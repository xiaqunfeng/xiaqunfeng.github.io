<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Dm-cache vs Bcache]]></title>
      <url>http://xiaqunfeng.github.io/2017/02/03/Dm-cache-vs-Bcache/</url>
      <content type="html"><![CDATA[<p>前面一篇文章介绍了flashcache和cache tier，并进行了对比，其实flashcache的原理和dm-cache是类似的。这里就介绍下dm-cache和bcache，这两种策略均已在linux内核中。<br><a id="more"></a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>现代硬盘驱动器（HDD）已经走了很长的路。他们旋转得越来越快;与过去的HDD相比具有更高的存储密度和更低的错误率，但是它们和其前身一样，仍然被视为性能瓶颈。随着时间的推移，我们已经看到了许多缓存方案来提高性能;有一些块缓存解决方案在磁盘驱动器（磁盘或存储控制器）或使用主机系统内存的文件系统级缓存解决方案附近使用某种类型的缓存。随着企业级的可接受性，大而经济的闪存存储技术的可用性，使用固态驱动器（SSD）的块缓存解决方案作为性能增强解决方案。使用基于SSD的块缓存，我们可以寻找SSD速度和HDD容量 - 快速，大而且实惠。</p>
<p>在块缓存解决方案中，概念上，将逻辑设备呈现给文件系统（或多个应用），而不是实际目的地HDD（或其它目的地块设备，例如iSCSI LUN），其中数据意在被存储。因此呈现的逻辑设备与原始设备具有相同的大小物理设备，而用于高速缓存的SSD可以跨越逻辑设备映射。</p>
<p>Linux开源社区有多个通用的块级缓存解决方案，其中包括bcache，dm-cache和flashcache。 Linux内核社区将内核3.9中的dm-cache上游和内核3.10中的bcache合并。虽然flashcache未在上游合并，但在某些生产环境中已经使用了一段时间。</p>
<h2 id="dm-cache"><a href="#dm-cache" class="headerlink" title="dm-cache"></a>dm-cache</h2><p>又叫 Device Mapper Cache。dm-cache是设备映射器目标，首先提交到kernel-3.9。它使用针对基于闪存的SSD进行优化的I / O调度和缓存管理技术。设备映射程序目标（dmcache）重新使用精简资源调配库中使用的元数据库。write-back和writethrough都由dm-cache支持，write-back是默认模式。</p>
<p>由dm-cache创建的虚拟缓存设备使用源设备（Origin device）、缓存设备（Cache device）和元数据设备（Metadata dev）这三个物理设备来构建。结构如下图所示：</p>
<p> <img src="http://ojet8bprn.bkt.clouddn.com/dm-cache.png" alt="dm-cache"></p>
<p>源设备是实际（较慢）目的存储设备。</p>
<p>高速缓存设备是用于临时存储用户数据的较快设备。</p>
<p>元数据设备记录块布局，它们的dirty flags和其他内部数据所需的策略。</p>
<h2 id="bcache"><a href="#bcache" class="headerlink" title="bcache"></a>bcache</h2><p>bcache首先提交到kernel-3.10。它围绕基于闪存的SSD的独特特性设计，并使用混合btree/log结构来跟踪缓存的区域。它旨在不惜一切代价（at all cost）避免随机写。 bcache顺序填充一个擦除块，然后在重新使用之前发出一个丢弃（缓存数据可以是桶上的任意一个扇区。bcache最大程度上减少了随机写的代价，它按顺序填充一个桶，重新使用时只需将桶设置为无效）。支持write-through和write-back高速缓存。write-back默认关闭，但可以在运行时任意打开和关闭。</p>
<p>通过后端设备（backing device）和缓存设备（caching device）来创建虚拟的bcache 设备，如下图所示：</p>
<p> <img src="http://ojet8bprn.bkt.clouddn.com/bcache.png" alt="bcache"></p>
<p>后端设备是实际（较慢）目的存储设备，而高速缓存设备是更快的设备。后备设备必须格式化为bcache 块设备；现有的格式化分区不能与bcache一起使用。（可以尝试 blocks to-bcache进行就地转换）。</p>
<p>默认状态下bcache不缓存顺序IO，只缓存随机读写。为避免随机写，bcache将随机写转换为顺序写，首先写到SSD，然后回写缓存使用SSD缓存大量的写，最后将写有序写到磁盘或者阵列上。</p>
<p>SSD的特点就是随机IO速度很快，而对于大块顺序IO的提升却并不大。bcache会检测顺序IO并忽略；还会对每一个任务记录动态的平均IO大小，当平均IO大小超过截止值时该任务后面的IO将会被忽略，这样就可以透传备份或者大文件拷贝。</p>
<h2 id="Dm-cache-vs-Bcache"><a href="#Dm-cache-vs-Bcache" class="headerlink" title="Dm-cache vs Bcache"></a>Dm-cache vs Bcache</h2><p>与 dm-cache 实现的的分级存储不同，bcache 更像一个传统的缓存。它可以用来存储任何 extents，甚至是是一个扇区，而 dm-cache 只能对整块数据进行缓存。</p>
<p>dm-cache方案假设缓存设备总是存在，bcache 并不要求 cache 设备一直都在。 </p>
<p>bcache 已经在实际产品中使用了，所以它有机会去碰到这些疑难场景，并可以处理这些缓存设备无法工作的情形。</p>
<p>dm-cache 确实还有很多事情要做。起初，它是进行 cache 和原始设备的并行 IO，但最终不得不回到顺序 IO。</p>
<h2 id="打赏通道"><a href="#打赏通道" class="headerlink" title="打赏通道"></a>打赏通道</h2><p align="center">如果本文对你有所帮助，欢迎小额赞助。您的打赏也是我坚持随笔总结的动力！</p><br><div style="text-align: center"><img src="http://ojet8bprn.bkt.clouddn.com/%E6%89%93%E8%B5%8F1.jpg"></div><br><p align="center">谢谢O(∩_∩)O~</p> 
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Flashcache vs Cache tier]]></title>
      <url>http://xiaqunfeng.github.io/2017/02/03/Flashcache-vs-Cache-tier/</url>
      <content type="html"><![CDATA[<p>使用SSD作为缓存来提高性能有多种方法，文章前言里会介绍，主要有bcache、dm-cache、flashcache、cache tier等，本文比较两种在ceph中的常用方案：flashcache 和 cache tier。<br><a id="more"></a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>传统的HDD具备大容量的优势，但是性能相对偏低，尤其是随机IO性能，经常成为系统的性能瓶颈，在虚拟机化环境下表现的更明显，因为虚拟化场景会加剧IO随机化。相比HDD，SSD具有高性能的优势，尤其在随机IO这方面，优势非常明显，但是SSD的硬件成本比较高。</p>
<p>目前业界在结合HDD的大容量和SSD的高性能方面做了一些优化，基本思路是使用SSD作为HDD的cache。计算机领域，cache的思想无处不在，比如CPU的L1、L2 cache，raid card上的cache，TLB cache等。关于SSD作为HDD的cache的优化方案，主要有Linux bcache，Linux dm-cache，Facebook flashcache，btier，IBM flashcache等。</p>
<h2 id="flashcache"><a href="#flashcache" class="headerlink" title="flashcache"></a>flashcache</h2><p>flashcache 是 facebook 开源的 ssd 存储产品，它基于内核的 devicemapper 机制，允许将 ssd 设备映射为机械存储设备的缓存，堆叠成为一个虚拟设备供用户读写，从而在一定程度上兼顾 ssd 的高速与机械存储设备的高容量，更加经济高效地支撑线上业务。</p>
<p> <img src="http://ojet8bprn.bkt.clouddn.com/flashcache.png" alt="flashcache"></p>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><p>Device Mapper层，对外服务体现方式是块设备，无需修改应用。</p>
<p>确保内核已启用 DeviceMapper，编译内核模块并安装之后：</p>
<ul>
<li>创建缓存设备: flashcache_wt_create /dev/cachedev /dev/sda1 /dev/hda1</li>
<li>删除缓存设备: dmsetup remove cachedev </li>
</ul>
<p>使用者需要注意建立缓存设备之后，就不应该再对 /dev/sda1 和 /dev/hda1 两个设备直接读写了。</p>
<p>用户操作接口是映射之后的虚拟设备，即/dev/mapper/*下面的设备，如cachedev，而不是基设备</p>
<h3 id="缓存的组织"><a href="#缓存的组织" class="headerlink" title="缓存的组织"></a>缓存的组织</h3><p>flashcache是建立在Linux devicemapper之上的，由devicemapper在SSD和backing HDD之上创建了一个逻辑的mapped device，用户使用的就是这个mapped device。</p>
<p>flashcache把cache（SSD）按照哈希的方式进行结构化管理，flashcache 中缓存的盘块的默认大小为 4kb，按照多路组相连的形式组织缓存块，每组含有 512 个缓存块，简单按照取模分组。缓存的查找，就是对后端设备块号取模得到组号，然后凭偏移遍历组中的缓存块。具体管理方式可以参考小结末参考资料。</p>
<p>虽然数据会被写入 ssd 设备，但缓存的组织信息一律保存在内存中，即 cache_c 对象。它里面保存了基本的配置选项、统计信息、对两个设备的引用、每个缓存块的相关信息(cache, cache_state)、 以及缓存组中 FIFO 数组(虽然名字叫做 LRU)的下标。</p>
<h3 id="缓存策略"><a href="#缓存策略" class="headerlink" title="缓存策略"></a>缓存策略</h3><p>flashcache 支持三种缓存策略：</p>
<ul>
<li>writethrough：disk write会在cache中保留一份，但同时也会把数据write到backing disk中，直到write backing disk完成才会返回。</li>
<li>writearound：disk write会bypass cache，直接写到backing disk中，disk read会把从backing disk中读取的数据在cache中缓存。</li>
<li>writeback：write首先会写到cache中，然后更新metadata中得dirty bit，数据并不会立即同步到backing disk中。</li>
</ul>
<p>针对writethrough和writearound，disk read首先根据目的sector找到对应的cache set，然后查找有没有相应的block，如果找到了，也就是cache hit，则直接从cache中读取，如果没有找到，就从backing disk中读取数据，同时也在cache中进行缓存。</p>
<h3 id="数据调度"><a href="#数据调度" class="headerlink" title="数据调度"></a>数据调度</h3><p>DM层request送往flashcache，并将读写请求按照目标设备的blocksize进行划分，切割成blocksize大小的bio。</p>
<p><strong>读</strong></p>
<p>接收到bio，首先，根据bio-&gt;bi_sector，即硬盘的扇区号，得到SSD上的set；</p>
<p>其次，在set内查找是否命中，如果命中，则将硬盘的扇区号转换为SSD的扇区号，然后将此bio向SSD提交，进行读取；</p>
<p>如果未命中，则首先向硬盘驱动提交bio，从硬盘读数据，读取完成后，由回调函数启动回写SSD操作，将bio的扇区号转换为SSD的=扇区号，然后向SSD驱动程序提交，将硬盘读取的数据写入SSD。</p>
<p><strong>写</strong></p>
<p>同文件系统页缓冲，并不直接写入硬盘，而是写入SSD，同时，保持一个阀值，一般为20%，在脏块数目达到此数值时，写回磁盘。</p>
<p>另外：如果<strong>IO size小于blocksize</strong>，flashcache则不会缓存该IO，而是先查找cache中有没有overlap的脏数据：</p>
<p>如果有的话，就先刷脏数据，然后再把刚才从devicemapper传来的IO写到backing disk上</p>
<p>如果没有脏数据，则直接写到backing disk上，这也就是为什么当使用fio测试小于4KB的随机IO时，flashcache几乎没有效果。</p>
<p>部分参考资料：</p>
<p><a href="http://mogu.io/Facebook_flashcache-81" target="_blank" rel="external">Facebook flashcache介绍与使用</a></p>
<p><a href="https://www.ibm.com/developerworks/community/blogs/5144904d-5d75-45ed-9d2b-cf1754ee936a/entry/flashcache-introduction?lang=en" target="_blank" rel="external">flashcache 简介</a></p>
<p><a href="http://blog.csdn.net/kidd_3/article/category/905673" target="_blank" rel="external">其他flashcache资料</a></p>
<h2 id="cache-tier"><a href="#cache-tier" class="headerlink" title="cache tier"></a>cache tier</h2><p>缓存分层通过将一部分数据存到缓存层，给ceph客户端提供更好的IO性能。通常是在更快的磁盘上创建存储池，如SSD/NVMe。数据最终被透明的写入常规的存储池中，它们由副本或纠删码类型的存储池所构建。</p>
<p><img src="http://ojet8bprn.bkt.clouddn.com/cache%20tier.png" alt="cache tier"></p>
<p>缓存层代理自动处理缓存层和后端存储之间的数据迁移，对客户端透明，管理员可以配置迁移进行的方式。</p>
<h3 id="两种模式"><a href="#两种模式" class="headerlink" title="两种模式"></a>两种模式</h3><p>缓存分层主要有两种模式：writeback 和 read-only。如果是write-back模式，那么该cache pool 既是read tier ，又是 write tier； 如果只是read only 模式，那么实际上，cache pool 只是 read tier，没有write tier。</p>
<h3 id="writeback模式"><a href="#writeback模式" class="headerlink" title="writeback模式"></a>writeback模式</h3><p>客户端写数据到缓存层后立刻收到确认回复。基于配置的flush/evict策略，数据从缓存层迁移到存储层，最后缓存分层代理从缓存层中将数据删除。</p>
<p>当读操作在缓存中miss的时候</p>
<p>1、返回client结果为cache miss，重新向存储层发起读请求。</p>
<p>2、cache层向存储层发起读请求，返回client的同时缓存一份在cache层</p>
<p>写操作在缓存中miss的时候</p>
<p>直接向存储层写，然后逐级返回，并不将数据缓存在cache层</p>
<p>这种模式适合大量修改数据的场景。</p>
<h3 id="read-only模式"><a href="#read-only模式" class="headerlink" title="read-only模式"></a>read-only模式</h3><p>缓存层只服务于客户端的读操作。当读请求在缓存层中miss的时候，从存储层中读上来，并设置一个expire时间，过期自动被删除。cache层可以设置为单副本，极大减少缓存空间占用率。写操作直接写到后端的存储层。</p>
<p>这种模式适合一次写入多次读取的场景。</p>
<h2 id="Flashcache-vs-Cache-tier"><a href="#Flashcache-vs-Cache-tier" class="headerlink" title="Flashcache vs Cache tier"></a>Flashcache vs Cache tier</h2><p>flashcache将缓存在块层的东西，而缓存池将缓存对象层的东西。 在每种情况下都有潜在的优点和缺点。 一些初始猜测：</p>
<p><strong>flashcache</strong></p>
<ul>
<li>[x] 所有缓存对一个节点是本地的，更少的开销（包括网络）。</li>
<li>[x] 在许多对象访问和大量热的inodes / dentries的情况下可以做得更好</li>
<li>无法仅为高速缓存定义不同的 副本/ EC 策略。</li>
<li>缓存写入是非原子的</li>
</ul>
<p><strong>Ceph cache tier</strong></p>
<ul>
<li>[x] 潜在可以做的事情，如副本缓存和EC存冷数据。</li>
<li>[x] 更安全</li>
<li>更多的网络开销，可能会有更多的CPU开销。</li>
<li>需要更长的时间才能将内容缓存</li>
</ul>
<p>参考资料：<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2014-March/037861.html" target="_blank" rel="external"> OSD + FlashCache vs. Cache Pool for RBD</a></p>
<p><strong>关于成本</strong></p>
<p>高速缓存中的数据总是硬盘驱动器上的数据的第二个副本。 缓存的故障很少产生数据丢失，只是性能损失，因为一切都需要从机械驱动器提供，直到缓存可以被替换。</p>
<p> SSD层可能是自动分层系统中唯一的数据副本，不能容忍SSD层的故障，因此这些系统必须通过使用类似RAID的数据保护方案在冗余配置中设置SSD层。 不得不购买额外的SSD来支持类似RAID的功能，使已经高价的技术更加昂贵。</p>
<p><strong>关于读</strong></p>
<p>在大多数情况下，两个选项之间的read 性能应该大致相同。 大多数情况下，read 性能的效率将取决于缓存设备提升数据的效率和可定制性。 目标应该是确保正确的数据在正确的时间存在缓存中。缓存和自动分层需要在关于他们缓存什么和什么时候缓存需要变得更加聪明。</p>
<p><strong>终上所述</strong></p>
<p>从用户的角度来看，如果适当的冗余存在，缓存或分层没有显着的优势，因此不应该单独选择基于闪存的存储系统。 IT规划人员可能想要调查其他因素，例如将某些数据集固定到闪存层或缓存区，以及与环境或应用程序集成的能力。</p>
<p>参考资料：（这两篇文章是同一个作者）</p>
<p><a href="http://www.networkcomputing.com/storage/ssd-options-tier-vs-cache/1267262167" target="_blank" rel="external">SSD Options: Tier Vs. Cache</a></p>
<p><a href="http://searchsolidstatestorage.techtarget.com/tip/Tiering-vs-caching-in-flash-based-storage-systems" target="_blank" rel="external">Tiering vs. caching in flash-based storage systems</a></p>
<h2 id="打赏通道"><a href="#打赏通道" class="headerlink" title="打赏通道"></a>打赏通道</h2><p align="center">如果本文对你有所帮助，欢迎小额赞助。您的打赏也是我坚持随笔总结的动力！</p><br><div style="text-align: center"><img src="http://ojet8bprn.bkt.clouddn.com/%E6%89%93%E8%B5%8F1.jpg"></div><br><p align="center">谢谢O(∩_∩)O~</p> 

]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux命令——parted]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/25/Linux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94parted/</url>
      <content type="html"><![CDATA[<p>一直用parted命令来进行磁盘分区，这里学习一下总结一下。顺带对比一下MBR和GPT两种分区表，以及parted命令和fdisk命令的区别。<br><a id="more"></a></p>
<h2 id="MBR和GPT"><a href="#MBR和GPT" class="headerlink" title="MBR和GPT"></a>MBR和GPT</h2><p>MBR：MBR分区表(即主引导记录)大家都很熟悉。所支持的最大卷：2T，而且对分区有限制：最多4个主分区或3个主分区加一个扩展分区（包含随意数目的逻辑分区）。</p>
<p>GPT：GPT（即GUID分区表）。是源自EFI标准的一种较新的磁盘分区表结构的标准，是未来磁盘分区的主要形式。GPT的分区信息是在分区中，而MBR在主引导扇区。</p>
<p>与MBR分区方式相比，具有如下优点：</p>
<ul>
<li>突破MBR 4个主分区限制，每个磁盘最多支持128个主分区。</li>
<li>支持大于2T的分区，最大卷可达18EB。</li>
</ul>
<h2 id="主分区、扩展分区、逻辑分区"><a href="#主分区、扩展分区、逻辑分区" class="headerlink" title="主分区、扩展分区、逻辑分区"></a>主分区、扩展分区、逻辑分区</h2><ul>
<li><strong>主分区</strong>又叫主磁盘分区，和扩展分区、逻辑分区一样，是一种分区类型。主分区中不能再划分其他类型的分区，因此每个主分区都相当于一个逻辑磁盘。一个硬盘的主分区也就是包含操作系统启动所必需的文件和数据的硬盘分区，要在硬盘上安装操作系统，则该硬盘必须得有一个主分区。</li>
<li><strong>扩展分区</strong>不能直接使用，必须分成若干逻辑分区。所有的逻辑分区都是扩展分区的一部分。</li>
<li><strong>逻辑分区</strong>也相当于一个逻辑磁盘，但主分区是直接在硬盘上划分的，逻辑分区则必须建立于扩展分区中。</li>
</ul>
<p>MBR模式分区只能划分四个分区，现在GPT分区至少可以划分128个主分区，未来很有可能将不存在扩展分区和逻辑分区的概念。</p>
<h2 id="parted和fdisk"><a href="#parted和fdisk" class="headerlink" title="parted和fdisk"></a>parted和fdisk</h2><p>parted命令可以划分单个分区大于2T的GPT格式的分区，也可以划分普通的MBR分区，并且允许调整分区的大小。</p>
<p>fdisk命令对于大于2T的分区无法划分，所以用fdisk无法看到parted划分的GPT格式的分区。</p>
<p>parted是一个可以分区并进行分区调整的工具，他可以创建，破坏，移动，复制，调整ext2 linux-swap fat fat32 reiserfs类型的分区，可以创建，调整，移动Macintosh的HFS分区，检测jfs，ntfs，ufs，xfs分区。</p>
<h2 id="parted命令"><a href="#parted命令" class="headerlink" title="parted命令"></a>parted命令</h2><p>Parted 命令分为两种模式：命令行模式和交互模式</p>
<p> 1、命令行模式：<code>parted [option] device [command]</code> ，该模式可以直接在命令行下对磁盘进行分区操作，比较适合编程应用，比如利用脚本批量执行命令。</p>
<p> 2、交互模式：<code>parted [option] device</code>类似于使用<code>fdisk /dev/xxx</code>，这种适合当个操作。</p>
<h3 id="命令行模式"><a href="#命令行模式" class="headerlink" title="命令行模式"></a>命令行模式</h3><h4 id="1、初始状态"><a href="#1、初始状态" class="headerlink" title="1、初始状态"></a>1、初始状态</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># lsblk</div><div class="line">NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</div><div class="line">sr0     11:0    1   436K  0 rom  </div><div class="line">vda    253:0    0    20G  0 disk </div><div class="line">├─vda1 253:1    0     2M  0 part </div><div class="line">├─vda2 253:2    0   476M  0 part /boot</div><div class="line">└─vda3 253:3    0  19.5G  0 part /</div><div class="line">vdb    253:16   0   100G  0 disk</div></pre></td></tr></table></figure>
<p>parted有个不提示用户参数选项，就是通过这个选项来实现非交互</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">-s, --script</div><div class="line">              never prompts for user intervention</div></pre></td></tr></table></figure>
<h4 id="2、添加分区"><a href="#2、添加分区" class="headerlink" title="2、添加分区"></a>2、添加分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># parted -s /dev/vdb mklabel gpt			# 定义分区表格式</div><div class="line"># parted -s /dev/vdb mkpart primary ext4 1 10G		# 划分主分区</div><div class="line"># parted -s /dev/vdb mkpart logic 10G 20G		# 划分逻辑分区</div><div class="line"># parted -s /dev/vdb mkpart logic ext3 20G 40G</div><div class="line"># parted -s /dev/vdb mkpart entended 40G 100%		# 划分扩展分区，100%表示使用剩余所有空间</div><div class="line"></div><div class="line"># parted -s /dev/vdb p					# 查看分区详情</div><div class="line">Model: Virtio Block Device (virtblk)</div><div class="line">Disk /dev/vdb: 107GB</div><div class="line">Sector size (logical/physical): 512B/512B</div><div class="line">Partition Table: gpt</div><div class="line"></div><div class="line">Number  Start   End     Size    File system  Name     Flags</div><div class="line"> 1      1049kB  10.0GB  9999MB  ext4         primary</div><div class="line"> 2      10.0GB  20.0GB  9999MB  xfs          logic</div><div class="line"> 3      20.0GB  40.0GB  20.0GB  ext3         logic</div><div class="line"> 4      40.0GB  107GB   67.4GB               entended</div></pre></td></tr></table></figure>
<p>查看分区后情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># lsblk</div><div class="line">NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</div><div class="line">sr0     11:0    1   436K  0 rom  </div><div class="line">vda    253:0    0    20G  0 disk </div><div class="line">├─vda1 253:1    0     2M  0 part </div><div class="line">├─vda2 253:2    0   476M  0 part /boot</div><div class="line">└─vda3 253:3    0  19.5G  0 part /</div><div class="line">vdb    253:16   0   100G  0 disk </div><div class="line">├─vdb1 253:17   0   9.3G  0 part </div><div class="line">├─vdb2 253:18   0   9.3G  0 part </div><div class="line">├─vdb3 253:19   0  18.6G  0 part </div><div class="line">└─vdb4 253:18   0  62.8G  0 part</div></pre></td></tr></table></figure>
<h4 id="3、删除分区"><a href="#3、删除分区" class="headerlink" title="3、删除分区"></a>3、删除分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># parted -s /dev/vdb rm 2</div><div class="line"></div><div class="line"># parted -s /dev/vdb p</div><div class="line">Model: Virtio Block Device (virtblk)</div><div class="line">Disk /dev/vdb: 107GB</div><div class="line">Sector size (logical/physical): 512B/512B</div><div class="line">Partition Table: gpt</div><div class="line"></div><div class="line">Number  Start   End     Size    File system  Name     Flags</div><div class="line"> 1      1049kB  10.0GB  9999MB  ext4         primary</div><div class="line"> 3      20.0GB  40.0GB  20.0GB  ext3         logic</div></pre></td></tr></table></figure>
<h3 id="交互模式"><a href="#交互模式" class="headerlink" title="交互模式"></a>交互模式</h3><h4 id="1、初始状态-1"><a href="#1、初始状态-1" class="headerlink" title="1、初始状态"></a>1、初始状态</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># lsblk</div><div class="line">NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</div><div class="line">sr0     11:0    1   436K  0 rom  </div><div class="line">vda    253:0    0    20G  0 disk </div><div class="line">├─vda1 253:1    0     2M  0 part </div><div class="line">├─vda2 253:2    0   476M  0 part /boot</div><div class="line">└─vda3 253:3    0  19.5G  0 part /</div><div class="line">vdb    253:16   0   100G  0 disk</div></pre></td></tr></table></figure>
<h4 id="2、添加分区-1"><a href="#2、添加分区-1" class="headerlink" title="2、添加分区"></a>2、添加分区</h4><p>针对盘 vdb 开始分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"># parted /dev/vdb</div><div class="line">GNU Parted 2.3</div><div class="line">Using /dev/vdb</div><div class="line">Welcome to GNU Parted! Type &apos;help&apos; to view a list of commands.</div><div class="line">(parted)mklabel gpt 				# 定义分区表格式</div><div class="line">...</div><div class="line">(parted) mkpart p1      			# 创建第一个分区，名称为p1                                                  </div><div class="line">File system type?  [ext2]? ext4     		# 定义分区格式                                      </div><div class="line">Start? 1 					# 定义分区的起始位置（单位支持K,M,G,T）                                                              </div><div class="line">End? 10G      					# 定义分区的结束为止(单位支持K,M,G,T)                                                            </div><div class="line">(parted) print     				# 查看当前分区情况                                                       </div><div class="line">Model: Virtio Block Device (virtblk)</div><div class="line">Disk /dev/vdb: 107GB</div><div class="line">Sector size (logical/physical): 512B/512B</div><div class="line">Partition Table: gpt</div><div class="line"></div><div class="line">Number  Start   End     Size    File system  Name  Flags</div><div class="line"> 1      1049kB  10.0GB  9999MB               p1</div><div class="line"></div><div class="line">(parted) mkpart part2                                                    </div><div class="line">File system type?  [ext2]? ext3                                           </div><div class="line">Start? 10G                                                                </div><div class="line">End? 20G                                                                  </div><div class="line">(parted) p					# print可以简写成“p“                                                            </div><div class="line">Model: Virtio Block Device (virtblk)</div><div class="line">Disk /dev/vdb: 107GB</div><div class="line">Sector size (logical/physical): 512B/512B</div><div class="line">Partition Table: gpt</div><div class="line"></div><div class="line">Number  Start   End     Size    File system  Name   Flags</div><div class="line"> 1      1049kB  10.0GB  9999MB               p1</div><div class="line"> 2      10.0GB  20.0GB  9999MB               part2</div><div class="line"></div><div class="line">(parted) quit					# 退出。quit可以简写为”q“ </div><div class="line">Information: You may need to update /etc/fstab.</div></pre></td></tr></table></figure>
<h4 id="3、设置文件系统"><a href="#3、设置文件系统" class="headerlink" title="3、设置文件系统"></a>3、设置文件系统</h4><p>不知道为啥，print的时候不现实文件系统格式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># mkfs.ext4 /dev/vdb1</div><div class="line"># mkfs.xfs /dev/vdb2</div></pre></td></tr></table></figure>
<p>再查看文件系统信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">(parted) print                                                            </div><div class="line">Model: Virtio Block Device (virtblk)</div><div class="line">Disk /dev/vdb: 107GB</div><div class="line">Sector size (logical/physical): 512B/512B</div><div class="line">Partition Table: gpt</div><div class="line"></div><div class="line">Number  Start   End     Size    File system  Name   Flags</div><div class="line"> 1      1049kB  10.0GB  9999MB  ext4         p1</div><div class="line"> 2      10.0GB  20.0GB  9999MB  xfs          part2</div></pre></td></tr></table></figure>
<p>发现分区的 File system 变成了 ext4 和 xfs。所以，还是通过命令的方式来定义文件系统格式。</p>
<p>查看一下分区后情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># lsblk</div><div class="line">NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</div><div class="line">sr0     11:0    1   436K  0 rom  </div><div class="line">vda    253:0    0    20G  0 disk </div><div class="line">├─vda1 253:1    0     2M  0 part </div><div class="line">├─vda2 253:2    0   476M  0 part /boot</div><div class="line">└─vda3 253:3    0  19.5G  0 part /</div><div class="line">vdb    253:16   0   100G  0 disk </div><div class="line">├─vdb1 253:17   0   9.3G  0 part </div><div class="line">└─vdb2 253:18   0   9.3G  0 part</div></pre></td></tr></table></figure>
<h4 id="4、删除分区"><a href="#4、删除分区" class="headerlink" title="4、删除分区"></a>4、删除分区</h4><p>删除分区 2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">(parted) rm                                                             </div><div class="line">Partition number? 2  </div><div class="line"></div><div class="line"># 或者直接</div><div class="line">(parted) rm 2    </div><div class="line"></div><div class="line">(parted) print</div><div class="line">Model: Virtio Block Device (virtblk)</div><div class="line">Disk /dev/vdb: 107GB</div><div class="line">Sector size (logical/physical): 512B/512B</div><div class="line">Partition Table: gpt</div><div class="line"></div><div class="line">Number  Start   End     Size    File system  Name  Flags</div><div class="line"> 1      1049kB  10.0GB  9999MB  ext4         p1</div></pre></td></tr></table></figure>
<h4 id="5、修改分区大小"><a href="#5、修改分区大小" class="headerlink" title="5、修改分区大小"></a>5、修改分区大小</h4><p>暂时不支持改变分区的大小（resize操作），针对有无文件系统报错如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># none</div><div class="line">Error: Could not detect file system.  </div><div class="line"></div><div class="line"># ext4，ext3</div><div class="line">Error: File system has an incompatible feature enabled.  Compatible       </div><div class="line">features are has_journal, dir_index, filetype, sparse_super and large_file.</div><div class="line">Use tune2fs or debugfs to remove features.</div><div class="line"></div><div class="line"># xfs</div><div class="line">No Implementation: Support for opening xfs file systems is not implemented</div><div class="line">yet.</div></pre></td></tr></table></figure>
<h2 id="打赏通道"><a href="#打赏通道" class="headerlink" title="打赏通道"></a>打赏通道</h2><p align="center">如果本文对你有所帮助，欢迎小额赞助。您的打赏也是我坚持随笔总结的动力！</p><br><div style="text-align: center"><img src="http://ojet8bprn.bkt.clouddn.com/%E6%89%93%E8%B5%8F1.jpg"></div><br><p align="center">谢谢O(∩_∩)O~</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[认识SSD]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/25/%E8%AE%A4%E8%AF%86SSD/</url>
      <content type="html"><![CDATA[<p>SSD越来越火，性能提升比较快，成本也有所下降，是未来的方向和主流，这里认识和学习一下SSD相关知识，以便更好的理解和应用SSD的特性。<br><a id="more"></a></p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>固态硬盘（Solid State Disk）用固态电子存储芯片阵列而制成的硬盘，由控制单元和存储单元（FLASH芯片、DRAM芯片）组成。<br>一块SSD，<strong>核心部件</strong>是一颗控制器和多颗Flash颗粒，一般还会有一些内存和超级电容。</p>
<pre><code>--&gt;控制器主要作用是提供高效的手段来访问这些Flash颗粒。
--&gt;内存的作用主要是表项管理和读写CACHE。
--&gt;超级电容的作用是在异常下电时保证关键数据不丢失。
</code></pre><p>而<strong>Flash颗粒</strong>是SSD最重要的器件，其相关特性决定了SSD的实现方式。</p>
<h2 id="FLASH颗粒"><a href="#FLASH颗粒" class="headerlink" title="FLASH颗粒"></a>FLASH颗粒</h2><h3 id="定义和分类"><a href="#定义和分类" class="headerlink" title="定义和分类"></a>定义和分类</h3><blockquote>
<p>闪存是一种不挥发性（ Non-Volatile ）内存，在没有电流供应的条件下也能够长久地保持数据，其存储特性相当于硬盘，这项特性正是闪存得以成为各类便携型数字设备的存储介质的基础。</p>
</blockquote>
<p>Flash颗粒是SSD最重要的器件，其相关特性决定了SSD的实现方式。目前业界主要有两种类型的Flash技术：NOR（Intel开发）和NAND（东芝开发）。由于这两种技术在如下方面的一些差异：</p>
<pre><code>--&gt;NOR的读取速度略快于NAND；
--&gt;NOR的写入速度远低于NAND；
--&gt;NOR的擦除速度远低于NAND；
--&gt;的擦写次数约为NAND的1/10；
--&gt;的单元尺寸约为NAND的2倍；
</code></pre><p>导致1~16M的闪存市场主要采用NOR颗粒（主要用于保存嵌入式设备的程序），而<strong>更大容量的数据存储（例如SSD）则主要选择NAND颗粒</strong>。</p>
<h3 id="SLC和MLC"><a href="#SLC和MLC" class="headerlink" title="SLC和MLC"></a>SLC和MLC</h3><p>NAND FLASH芯片，flash颗粒有两种类型：SLC和MLC</p>
<p>SLC：Single Level Cell，每个存储单元均只能存储1个bit的数据。只包含0和1两个电压符。<br>MLC：Multi Level Cell，每个存储单元能够存储2个bit或者更多bit的数据。包含四个电压符（00,01,10,11）。</p>
<p><strong>SLC和MLC的比较：</strong><br>MLC容量比SLC大，且价格便宜。<br>但在性能、使用寿命和稳定性上：<br>1、SLC更简单可靠；<br>2、SLC读取和写入的速度都比MLC更快；<br>3、SLC比MLC更耐用，MLC每单元可擦除1w次，而SLC可擦除10w次。</p>
<p>一般而言，<strong>SLC主要用于企业级市场</strong>，MLC主要用于消费级市场。下表显示了SLC和MLC两种Flash颗粒的一些性能指标：</p>
<table>
<thead>
<tr>
<th>比较项</th>
<th>SLC</th>
<th>MLC</th>
</tr>
</thead>
<tbody>
<tr>
<td>随机读</td>
<td>25us</td>
<td>50us</td>
</tr>
<tr>
<td>擦除</td>
<td>2ms</td>
<td>2ms</td>
</tr>
<tr>
<td>编程</td>
<td>250us</td>
<td>900us</td>
</tr>
<tr>
<td>擦除次数</td>
<td>100k</td>
<td>10k</td>
</tr>
</tbody>
</table>
<h2 id="SSD的内部逻辑划分"><a href="#SSD的内部逻辑划分" class="headerlink" title="SSD的内部逻辑划分"></a>SSD的内部逻辑划分</h2><p>一颗NAND Flash颗粒，其内部被划分为若干block，每个block又包含若干page。page的大小一般为2k/4k/8k（同一颗Flash颗粒内的page大小是固定且相同的）。</p>
<p>Flash的一个特点是：不能对任意bit位进行0/1互转，只能按照一定的颗粒度进行擦除和编程操作；</p>
<pre><code>--&gt;擦除 —— 颗粒度为block，是将指定block中所有的bit位全部设置为1；
--&gt;编程 —— 颗粒度为page，是将指定page中指定的bit位设置为0；
</code></pre><p>因此，对SSD进行写的操作实际上对需要写的Page所在的Block全部完成“擦除”(全部bit位置1)后，才能对指定的Page进行“编程”(部分bit位置0)。PS：实际上SSD硬盘在出厂时，厂商都会把所有Block完成擦除操作。</p>
<h2 id="SSD的表项管理"><a href="#SSD的表项管理" class="headerlink" title="SSD的表项管理"></a>SSD的表项管理</h2><p>  每一个IO读写都是下发给一个地址，这个地址称为LBA(Logic Block Address)，其真正对应在磁盘上的地址称为PBA(Physical Block Address)，和传统机械硬盘中LBA到PBA的映射通过磁轨、磁道、扇区来进行唯一对应不同的是，<strong>SSD的LBA到PBA</strong>的映射通过一张<strong>映射表</strong>来记录。<br>&emsp;&emsp;LBA的粒度是扇区，而Flash颗粒的擦粒度是block，写粒度是page，均为扇区的若干倍，在这种条件下，如果仍以扇区为粒度进行映射，虽然理论上可行，但是显然是增加实现难度和性能的.<br>&emsp;&emsp;同时，如果强制以扇区为粒度进行映射，会导致所需要的映射表空间超过物理内存的限制，这就直接决定了不可能以扇区为粒度进行映射。<br>&emsp;&emsp;即使以page为粒度进行映射，也可能存在物理内存不够用的情况（现在正在使用的SSD盘即属于这种情况），所以需要以更大一些的粒度进行管理。<br>&emsp;&emsp;这个粒度成为”<strong>小块(Sub Block)</strong>“,是SSD进行擦除和编程的最小单位。每个block中，均包含多个sub block；每个sub block，由多个page组成；每个sub block，可能的状态包括：有效、垃圾、空白：</p>
<pre><code>--&gt;有效：该小块中的数据正在被引用；
--&gt;垃圾：该小块中的数据没有被引用；
--&gt;空白：该小块可用于写入新的数据；
</code></pre><h2 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h2><p>&emsp;&emsp;SSD的写要先进行整个Block的擦除，然后才能对指定的Sub Block进行编程。实际上，如果某一个写操作需要修改一个Sub Block的内容，是无法直接进行修改的，而是写到一个新的Sub Block中，把该LBA的映射指向新的这个Sub Block，同时把原来数据所在的Sub Block标记为“垃圾”。<br>  这样，随着SSD使用时间的越来越长，新的未被编程过的Sub Block越来越少，后面新的写请求则只能重新擦除所有Sub Block均为垃圾的Block才能再次写入，这样必然会造成写操作的性能降低，因此，在SSD内部引入了一个重要的机制——垃圾回收（Garbage Collection），用以提升SSD长期写入操作的性能。</p>
<p><strong>垃圾回收的工作：</strong></p>
<pre><code>--&gt;找出合适的block
--&gt;将该block中的有效小块迁移到别的地方，同时更改映射表
--&gt;将该block擦除，并放入空白块表
</code></pre><p>&emsp;&emsp;所谓“最合适进行回收的block”，是指这个block的垃圾小块足够多、有效/空白小块足够少。<br>&emsp;&emsp;垃圾回收算法是否高效，与该SSD的性能有很强的联系。</p>
<h2 id="SSD的磨损均衡"><a href="#SSD的磨损均衡" class="headerlink" title="SSD的磨损均衡"></a>SSD的磨损均衡</h2><p>  上面说过，每个Flash颗粒中Block的可擦除次数是有限制的，如果颗粒中的某些Block很快达到了擦除次数上限，而其他大部分都几乎没有被擦除过，则会很快的造成整个SSD失效。而磨损均衡（Wear Leveling）则是通过各种手段，保证整个SSD所有的block的擦除次数是相近的从而延长SSD的使用寿命。</p>
<p>磨损均衡分为  <strong>动态磨损均衡</strong>  和  <strong>静态磨损均衡</strong>：</p>
<blockquote>
<p>动态磨损均衡，是指在外部力量的驱动下，自然完成磨损均衡。这里外部力量，包括写IO和垃圾回收。<br>静态磨损均衡，是指磨损均衡功能模块主动地查找那些长时间没有变化的数据，将这些数据搬移到其他位置，以便释放出擦除次数较少的block，使其投入到擦除/编程的循环之中。</p>
</blockquote>
<h2 id="SSD使用寿命计算举例"><a href="#SSD使用寿命计算举例" class="headerlink" title="SSD使用寿命计算举例"></a>SSD使用寿命计算举例</h2><p>&emsp;&emsp;虽然一个block擦写次数只有100k（SLC），但是因为动态映射、磨损均衡等机制的存在，使得SSD的寿命远不止写入100k个IO。<br>计算SSD寿命的方式，一般是先确认一个block在其生命周期内能够写入多少数据量，再乘以该SSD具有的block数量，再除以一个估计的写入带宽，最后得到寿命值。<br>&emsp;&emsp;以我司（<strong>你们猜o(^▽^)o</strong>）自研的100G SLC SSD为例，16片Flash颗粒，每片颗粒拥有32k个block，每个block拥有64个4k的page，每个block可以被擦除100k次：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">16 x (32 x 1024) x 64 x (4 x 1024) x 100000 = 12500 TB</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;12500 TB，这是当所有block同时达到擦除次数上限时，可以写入的数据总量。<br>按照用户平均每秒钟写入10MB的数据进行计算：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(12500 x 1024 x 1024) / (10 x 60 x 60 x 24 x 365.25) = 41.5 年</div></pre></td></tr></table></figure></p>
<p>&emsp;&emsp;41.5年，这是在10MB/s的压力下、持续不断地向SSD写入数据的使用寿命。<br>&emsp;&emsp;考虑到其他一些因素的影响，一块SLC SSD的使用寿命，10年是可以保证的。</p>
<h2 id="SSD和传统磁盘的对比"><a href="#SSD和传统磁盘的对比" class="headerlink" title="SSD和传统磁盘的对比"></a>SSD和传统磁盘的对比</h2><p>&emsp;&emsp;SSD与传统磁盘相比:<br>&emsp;&emsp;第一是没有机械装置;<br>&emsp;&emsp;第二是由磁介质改为了电介质。<br>&emsp;&emsp;在SSD内部有一个FTL(Flash Transalation Layer)，它相当于磁盘中的控制器，主要功能就是作地址映射，将flash memory的物理地址映射为磁盘的LBA逻辑地址，并提供给OS作透明访问。</p>
<h3 id="读方面"><a href="#读方面" class="headerlink" title="读方面"></a>读方面</h3><p><strong>随机读</strong><br>  SSD没有传统磁盘的寻道时间和延迟时间，所以SSD可以提供非常高的随机读取能力。</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>SLC类型的SSD</td>
<td>超过35000的IOPS</td>
</tr>
<tr>
<td>传统15k的SAS磁盘</td>
<td>最高160个IOPS。</td>
</tr>
</tbody>
</table>
<p><strong>连续读</strong><br>  SSD连续读的能力相比普通磁盘优势并不明显。传统磁盘连续读，并不需要寻道时间：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>SLC类型的SSD</td>
<td>超过35000的IOPS</td>
</tr>
<tr>
<td>传统15k的SAS磁盘</td>
<td>最高160个IOPS。</td>
</tr>
</tbody>
</table>
<h3 id="写方面"><a href="#写方面" class="headerlink" title="写方面"></a>写方面</h3><p>  Page为最小的读写单位，Block为最小的擦除/编程单位，其中1个Page为4KB，1个Block由256个Page组成，1个Plane由2048个Block组成，2个Plane组成1个Die，也就是最小的芯片(4GB)</p>
<p>&emsp;&emsp;向一个空白的page写入信息时，可以直接写入而无需擦除，但是如果需要改写某个存储单元（page）的数据，必须首先将整个block读入缓存，然后修改数据，并擦除整个block的数据，最后将整个block写入。SSD改写数据的代价很高，SSD的这个特性，我们称之为erase-before-write。因为这个特性，引入<strong>“写放大”</strong>的概念。</p>
<p>&emsp;&emsp;比如你想改写4K的数据，必须首先将整个擦除块（1024KB）中的数据读出到缓存中，改写后，将整个块一起写入，这时你实际写入了1024KB的数据，写入放大系数是256。写入放大最好的情况是1，就是不存在放大的情况。</p>
<p>&emsp;&emsp;在<strong>长时间写入</strong>后，MLC随机写IO下降得非常厉害，而<strong>SLC表现则比较稳定</strong>，可以稳定在3000 IOPS，而MLC随机写IOPS甚至降低到300。</p>
<p>&emsp;&emsp;当某个单元长时间被反复擦写时（比如Oracle redo），不仅会造成写入的性能问题，而且会大大缩短SSD的使用寿命。所以，引入磨损均衡算法（wear leveling）。</p>
<h2 id="打赏通道"><a href="#打赏通道" class="headerlink" title="打赏通道"></a>打赏通道</h2><p align="center">如果本文对你有所帮助，欢迎小额赞助。您的打赏也是我坚持随笔总结的动力<br>！</p><br><div style="text-align: center"><img src="http://ojet8bprn.bkt.clouddn.com/%E6%89%93%E8%B5%8F1.jpg"></div><br><p align="center">谢谢O(∩_∩)O~</p> 
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Ceph — V11.2.0 KRAKEN(译)]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/24/Ceph-KRAKEN(V11.2.0)/</url>
      <content type="html"><![CDATA[<p>ceph的版本出到K了，针对bluestore又有了新的优化，并进行了一定的压力和故障测试，当然还有其他的特性，针对Jewel版本有较多改动。当前还没有什么中文资料，<a href="http://docs.ceph.org.cn/" target="_blank" rel="external">ceph中文社区</a>中关于V11.2.0还没有更新进去，所以闲暇之余翻译了一下，也给大家节省时间。<br><a id="more"></a></p>
<p>英文原文链接：<a href="http://docs.ceph.com/docs/master/release-notes/#v11.2.0-kraken" target="_blank" rel="external">http://docs.ceph.com/docs/master/release-notes/#v11.2.0-kraken</a></p>
<h2 id="V11-2-0-KRAKEN"><a href="#V11-2-0-KRAKEN" class="headerlink" title="V11.2.0 KRAKEN"></a>V11.2.0 KRAKEN</h2><p>这是Kraken系列的第一个release版本。它适用于生产部署，并将维护到下一个稳定的版本Luminous，将在2017年春季完成。</p>
<h2 id="对比JEWEL版本的主要变化"><a href="#对比JEWEL版本的主要变化" class="headerlink" title="对比JEWEL版本的主要变化"></a>对比JEWEL版本的主要变化</h2><h3 id="RADOS"><a href="#RADOS" class="headerlink" title="RADOS"></a>RADOS</h3><ul>
<li>新的BlueStore 后端现在有一个稳定的磁盘格式，并通过了我们的故障和压力测试。尽管该后端仍被标记为实验性的，但我们鼓励用户使用非生产集群和非关键数据集进行尝试。</li>
<li>RADOS现在已经对EC-pool上的覆盖写（overwrites）进行了实验性支持。因为磁盘格式和实现尚未完成，所以必须启用特殊pool选项才能测试新功能。在群集上启用此选项将永久禁止该群集升级到将来的版本。</li>
<li>我们现在默认使用AsyncMessenger（ms type = async）而不是传统的SimpleMessenger。最明显的区别是，我们现在使用固定大小的线程池用于网络连接（而不是使用SimpleMessenger的每个套接字两个线程）。</li>
<li>现在一些OSD的故障几乎可以立即检测到，而先前心跳超时（默认为20秒）必须到期。这可以防止IO在主机保持启动但ceph-osd进程不再运行的故障情况下长时间阻塞。</li>
<li>有一个新的ceph-mgr守护进程。它目前与默认监视器并置，并且尚未使用太多，但基本的基础设施现在已到位。</li>
<li>减少了编码OSDMaps的大小。</li>
<li>当恢复（recovery）或重新平衡（rebalanceing）正在进行时，OSD现在停止擦除（scrubbing）。</li>
</ul>
<h3 id="RGW"><a href="#RGW" class="headerlink" title="RGW"></a>RGW</h3><ul>
<li>RGW现在支持一种新的区域类型，可用于通过ElasticSearch进行元数据索引。</li>
<li>RGW现在支持S3复合对象复制部分API。</li>
<li>现在可以重新分割现有的桶。注意，桶重新分片目前需要停止对特定桶的所有IO（特别是写）。</li>
<li>RGW现在支持对象的数据压缩。</li>
<li>Civetweb版本已经升级到1.8。</li>
<li>现在支持Swift静态网站API（以前已经添加了S3支持）。</li>
<li>S3桶生命周期API已添加。注意，目前它只支持对象到期。</li>
<li>支持自定义搜索过滤器已添加到LDAP身份验证实现中。</li>
<li>支持NFS版本3已添加到RGW NFS网关。</li>
<li>为librgw创建了一个Python绑定。</li>
</ul>
<h3 id="RBD"><a href="#RBD" class="headerlink" title="RBD"></a>RBD</h3><ul>
<li>RBD现在支持使用新的(实验)覆盖写(overwrite)支持在 EC RADOS pool 中存储images。必须使用新的rbd CLI <code>“-data-pool &lt;ec pool&gt;”</code> 选项来创建images，用以指定EC池存储后端数据对象。尝试在EC池上直接创建image不会成功，因为image的后端元数据仅在副本池上被支持。</li>
<li>rbd-mirror守护进程现在支持从主image复制动态image特征更新和image元数据key/value对到非主image。</li>
<li>image快照的数量可以选择性地限制为可配置的最大值。</li>
<li>rbd Python API现在支持异步IO操作。</li>
</ul>
<h3 id="CephFS"><a href="#CephFS" class="headerlink" title="CephFS"></a>CephFS</h3><ul>
<li>libcephfs函数的定义已更改为启用适当的uid / gid控制。库版本已增加以反映接口更改。</li>
<li>备用的回放(replay)MDS守护程序现在在执行删除操作的工作负载上消耗更少的内存。</li>
<li>Scrub现在修复backtrace，并用已发现的错误填充损坏ls。</li>
<li>cephfs-data-scan的新的pg_files子命令可以识别受损坏或丢失的RADOS PG影响的文件。</li>
<li>假阳性“未能响应缓存压力”警告已修复。</li>
</ul>
<h2 id="UPGRADING-FROM-JEWEL"><a href="#UPGRADING-FROM-JEWEL" class="headerlink" title="UPGRADING FROM JEWEL"></a>UPGRADING FROM JEWEL</h2><ul>
<li>所有集群必须首先升级到 Jewel 10.2.z，然后升级到 Kraken 11.2.z（或者，最终，Luminous 12.2.z）。</li>
<li>升级到Kraken之前，必须在Jewel集群上设置sortbitwise标志。如果未设置标志，则最新的Jewel（10.2.4+）版本会发出健康警告，因此可能已设置。如果不是，Kraken OSDs将拒绝启动，并将在其日志中打印和错误消息。</li>
<li>您可以按任意顺序升级OSD，监视器和MDS。 RGW守护进程应该最后升级。</li>
<li>升级时，新的ceph-mgr守护进程实例将自动被创建，与任何监视器一起。在Jewel到Kraken和Jewel到Luminous的升级中这将是事实，但在未来高于Luminous版本的升级中可能不是这样。如果你不与他们共置ceph-mon守护进程的话，你当然可以自由创建新的ceph-mgr守护进程实例，并销毁自动创建的实例。</li>
</ul>
<h2 id="BLUESTORE"><a href="#BLUESTORE" class="headerlink" title="BLUESTORE"></a>BLUESTORE</h2><p>BlueStore是一个新的后端，用于管理直接硬盘或SSD上每个OSD存储的数据。与现有的FileStore实现（它利用XFS文件系统将对象存储为文件）不同，BlueStore直接管理底层块设备。实现它自己的文件系统类磁盘结构，专为Ceph OSD工作负载而设计。 BlueStore的主要特点包括：</p>
<ul>
<li>默认情况下启用写入磁盘的所有数据的校验和，所有读取的校验和验证。</li>
<li>内联压缩支持，可以分别通过池属性或客户端提示在每个池或每个对象的基础上启用。</li>
<li>高效日记。与将所有数据写入其日志设备的FileStore不同，BlueStore仅记录元数据和（在某些情况下）小型写入，从而减少其日志的大小和吞吐量要求。与文件存储一样，日志可以与其他数据共存在同一设备上，或者分配在较小的高性能设备（例如，SSD或NVMe设备）上。 BlueStore日志默认只有512 MB。</li>
</ul>
<p>BlueStore磁盘格式预计将继续发展。但是，我们将在OSD中提供支持，以便在升级时迁移到新格式。</p>
<p>为了启用BlueStore，将以下内容添加到ceph.conf：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">enable experimental unrecoverable data corrupting features = bluestore</div></pre></td></tr></table></figure>
<p>要创建一个BlueStore OSD，在OSD创建期间将-bluestore选项传递给ceph-disk或ceph-deploy。</p>
<h2 id="打赏通道"><a href="#打赏通道" class="headerlink" title="打赏通道"></a>打赏通道</h2><p align="center">如果本文对你有所帮助，欢迎小额赞助。您的打赏也是我坚持随笔总结的动力！</p><br><div style="text-align: center"><img src="http://ojet8bprn.bkt.clouddn.com/%E6%89%93%E8%B5%8F1.jpg"></div><br><p align="center">谢谢O(∩_∩)O~</p> 
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Crushmap学习]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/24/Crushmap%E5%AD%A6%E4%B9%A0/</url>
      <content type="html"><![CDATA[<p>介绍了ceph中crush的优点、原理及作用，同时dump出自己的crushmap内容，进行详细的分解学习。<br><a id="more"></a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>随着大规模分布式存储系统的出现。这些系统必须平衡的分布数据和负载，最大化系统的性能，并要处理系统的扩展和硬件失效。但是简单HASH分布不能有效处理设备数量的变化，导致大量数据迁移。ceph设计了CRUSH，用在分布式对象存储系统上，可以有效映射数据对象到存储设备上(不需要中心设备)。因为大型系统的结构式动态变化的，CRUSH能够处理存储设备的添加和移除，并最小化由于存储设备的的添加和移动而导致的数据迁移。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>CRUSH算法，全称 Controlled Replication Under Scalable Hashing。它是一种伪随机的算法，在相同的环境下，它能够在层级结构的存储集群中有效的分布对象的副本。</p>
<p>CRUSH实现了一种伪随机(确定性)的函数，它的参数是object id或object group id，并返回一组存储设备(用于保存object副本)。相似的输入得到的结果之间没有相关性，相同的输入得到的结果是确定的。</p>
<p>CRUSH需要cluster map(描述存储集群的层级结构)、和副本分布策略(rule)就可以根据一个整型的输入得到存放数据的一个设备列表。</p>
<p>CRUSH有两个关键优点：</p>
<ul>
<li>任何组件都可以独立计算出每个object所在的位置(去中心化)。</li>
<li>只需要很少的元数据(cluster map)，只要当删除添加设备时，这些元数据才需要改变。</li>
</ul>
<h2 id="crush的作用"><a href="#crush的作用" class="headerlink" title="crush的作用"></a>crush的作用</h2><p>传统存储的架构体系需要一个MDC（meta data controll），ceph不使用这种架构，而是通过引入CRUSH算法，将数据分布的查询操作变成了计算操作，并且是在client端完成。同时，Crush算法还有效缓解了普通hash算法在处理存储设备增删时带来的数据迁移问题。</p>
<p>ceph允许客户端直接访问OSD守护进程，从而消除集中式查询元数据中心的需求。ceph使用一个monitor集群来保证系统的高可用性。同时采用crush算法来实现去中心化，使用crush算法后，客户端根据需要来计算数据被写到哪里，以及从哪读数据。</p>
<p><strong>ceph集群的一个写操作流程</strong></p>
<p>1、client首先访问ceph monitor获取cluster map的一份副本，共五个map（monitor map，OSD map，MDS map，CRUSH map，PG map），知晓集群的状态和配置；</p>
<p>2、数据被转化为一个或多个对象，每个对象都具有对象名称和存储池名称；</p>
<p>3、以PG数为基数做hash，将对象映射到一个PG钟；</p>
<p>4、根据计算出的PG，再通过CRUSH算法得到存放数据的一组OSD位置（副本个数），第一个是主，后面是从；</p>
<p>5、客户端获得OSD ID，直接和这些OSD通信并存放数据。</p>
<p><strong>注</strong>：所有的这些操作都是在<strong>客户端</strong>完成的，不会影响ceph集群服务器端的性能。</p>
<p><strong>一句话描述</strong></p>
<p>说了半天，一句话说明CRUSH的作用，就是——<strong>根据PG ID得到一个OSD列表</strong>。</p>
<h2 id="crush的工作形式"><a href="#crush的工作形式" class="headerlink" title="crush的工作形式"></a>crush的工作形式</h2><p>CRUSH是基于一张描述当前集群资源状态的map（Crush map）按照一定的规则（rules）得到这个OSD列表的。Ceph将系统的所有硬件资源描述成一个树状结构，然后再基于这个结构按照一定的容错规则生成一个逻辑上的树形结构作为Crush map。数的叶子节点是OSD。</p>
<h2 id="实例讲解"><a href="#实例讲解" class="headerlink" title="实例讲解"></a>实例讲解</h2><h3 id="集群详情"><a href="#集群详情" class="headerlink" title="集群详情"></a>集群详情</h3><p>三个节点（ceph1，ceph2，ceph3），每个节点三个OSD，如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># ceph osd tree</div><div class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY </div><div class="line">-1 1.36345 root default                                     </div><div class="line">-2 0.45448     host ceph1                                   </div><div class="line"> 0 0.09090         osd.0       up  1.00000          1.00000 </div><div class="line"> 1 0.18179         osd.1       up  1.00000          1.00000 </div><div class="line"> 2 0.18179         osd.2       up  1.00000          1.00000 </div><div class="line">-3 0.45448     host ceph2                                   </div><div class="line"> 3 0.09090         osd.3       up  1.00000          1.00000 </div><div class="line"> 4 0.18179         osd.4       up  1.00000          1.00000 </div><div class="line"> 5 0.18179         osd.5       up  1.00000          1.00000 </div><div class="line">-4 0.45448     host ceph3                                   </div><div class="line"> 6 0.09090         osd.6       up  1.00000          1.00000 </div><div class="line"> 7 0.18179         osd.7       up  1.00000          1.00000 </div><div class="line"> 8 0.18179         osd.8       up  1.00000          1.00000</div></pre></td></tr></table></figure>
<h3 id="crushmap内容"><a href="#crushmap内容" class="headerlink" title="crushmap内容"></a>crushmap内容</h3><p>来看一下crushmap</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line"># begin crush map</div><div class="line">tunable choose_local_tries 0</div><div class="line">tunable choose_local_fallback_tries 0</div><div class="line">tunable choose_total_tries 50</div><div class="line">tunable chooseleaf_descend_once 1</div><div class="line">tunable chooseleaf_vary_r 1</div><div class="line">tunable straw_calc_version 1</div><div class="line"></div><div class="line"># devices</div><div class="line">device 0 osd.0</div><div class="line">device 1 osd.1</div><div class="line">device 2 osd.2</div><div class="line">device 3 osd.3</div><div class="line">device 4 osd.4</div><div class="line">device 5 osd.5</div><div class="line">device 6 osd.6</div><div class="line">device 7 osd.7</div><div class="line">device 8 osd.8</div><div class="line"></div><div class="line"># types</div><div class="line">type 0 osd</div><div class="line">type 1 host</div><div class="line">type 2 chassis</div><div class="line">type 3 rack</div><div class="line">type 4 row</div><div class="line">type 5 pdu</div><div class="line">type 6 pod</div><div class="line">type 7 room</div><div class="line">type 8 datacenter</div><div class="line">type 9 region</div><div class="line">type 10 root</div><div class="line"></div><div class="line"># buckets</div><div class="line">host ceph1 &#123;</div><div class="line">	id -2		# do not change unnecessarily</div><div class="line">	# weight 0.454</div><div class="line">	alg straw</div><div class="line">	hash 0	# rjenkins1</div><div class="line">	item osd.0 weight 0.091</div><div class="line">	item osd.1 weight 0.182</div><div class="line">	item osd.2 weight 0.182</div><div class="line">&#125;</div><div class="line">host ceph2 &#123;</div><div class="line">	id -3		# do not change unnecessarily</div><div class="line">	# weight 0.454</div><div class="line">	alg straw</div><div class="line">	hash 0	# rjenkins1</div><div class="line">	item osd.3 weight 0.091</div><div class="line">	item osd.4 weight 0.182</div><div class="line">	item osd.5 weight 0.182</div><div class="line">&#125;</div><div class="line">host ceph3 &#123;</div><div class="line">	id -4		# do not change unnecessarily</div><div class="line">	# weight 0.454</div><div class="line">	alg straw</div><div class="line">	hash 0	# rjenkins1</div><div class="line">	item osd.6 weight 0.091</div><div class="line">	item osd.7 weight 0.182</div><div class="line">	item osd.8 weight 0.182</div><div class="line">&#125;</div><div class="line">root default &#123;</div><div class="line">	id -1		# do not change unnecessarily</div><div class="line">	# weight 1.363</div><div class="line">	alg straw</div><div class="line">	hash 0	# rjenkins1</div><div class="line">	item ceph1 weight 0.454</div><div class="line">	item ceph2 weight 0.454</div><div class="line">	item ceph3 weight 0.454</div><div class="line">&#125;</div><div class="line"></div><div class="line"># rules</div><div class="line">rule replicated_ruleset &#123;</div><div class="line">	ruleset 0</div><div class="line">	type replicated</div><div class="line">	min_size 1</div><div class="line">	max_size 10</div><div class="line">	step take default</div><div class="line">	step chooseleaf firstn 0 type host</div><div class="line">	step emit</div><div class="line">&#125;</div><div class="line"></div><div class="line"># end crush map</div></pre></td></tr></table></figure>
<h3 id="begin-crush-map"><a href="#begin-crush-map" class="headerlink" title="begin crush map"></a>begin crush map</h3><p>配置参数</p>
<h3 id="devices"><a href="#devices" class="headerlink" title="devices"></a>devices</h3><p>列出集群的OSD设备</p>
<h3 id="types"><a href="#types" class="headerlink" title="types"></a>types</h3><p>表示buckets的类型 <img src="http://ojet8bprn.bkt.clouddn.com/osdmap.png" alt="osdmap"></p>
<h3 id="Buckets"><a href="#Buckets" class="headerlink" title="Buckets"></a>Buckets</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">host ceph1 &#123;</div><div class="line">	id -2		# do not change unnecessarily</div><div class="line">	# weight 0.454</div><div class="line">	alg straw</div><div class="line">	hash 0	# rjenkins1</div><div class="line">	item osd.0 weight 0.091</div><div class="line">	item osd.1 weight 0.182</div><div class="line">	item osd.2 weight 0.182</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>具体的定义是这样</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[bucket-type] [bucket-name] &#123;</div><div class="line">  	id 		[a unique negative numeric ID]</div><div class="line">  	weight	[the relative capacity the item]</div><div class="line">  	alg		[the bucket type: uniform | list | tree | straw | straw2]</div><div class="line">  	hash	[the hash type: 0 by default]</div><div class="line">  	item	[item-name]		weight		[weight]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Bucket-type: bucket的类型，用来指定OSD在CRUSH分层结构中的位置</p>
<p>bucket-name：唯一的bucket名称</p>
<p>id：唯一的ID，用一个负整数表示</p>
<p>weight：OSD的权重，权重越高，说明它的物理存储容量越大。建议1TB存储设备的权重为1.00，500G的权重为0.50。当然这是一个相对值。</p>
<p>alg：bucket类型的算法选择</p>
<blockquote>
<p>见最后bucket的类型</p>
</blockquote>
<p>hash：每个bucket都具有hash算法。目前ceph支持jrenkins1算法，默认hash设置为0使用该算法。</p>
<p>item：bucket里包含的元素，即叶子bucket，及其权重。这里bucket为host，叶子为OSD。</p>
<h3 id="rules"><a href="#rules" class="headerlink" title="rules"></a>rules</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">rules</div><div class="line">rule replicated_ruleset &#123;</div><div class="line">	ruleset 0</div><div class="line">	type replicated</div><div class="line">	min_size 1</div><div class="line">	max_size 10</div><div class="line">	step take default</div><div class="line">	step chooseleaf firstn 0 type host</div><div class="line">	step emit</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>具体定义</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">rule [rulename] &#123;</div><div class="line">  	ruleset		[ruleset]</div><div class="line">  	type		[replicated | erasure]</div><div class="line">  	min_size	[min-size]</div><div class="line">  	max_size	[max-size]</div><div class="line">  	step take	[bucket-type]</div><div class="line">  	step		[choose | chooseleaf]	firstn	[num]	type	[bucket-type]</div><div class="line">  	step emit</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>rulename：定义的规则名称</p>
<p>ruleset：一个整数值，指定这条规则所属的规则集</p>
<p>min_size和max_size用来限定这个rule的使用范围，即当一个pool的副本数小于min_size或者大于max_size的时候不使用这个rule。</p>
<p>step take：选择一个bucket，然后从这个bucket开始往下遍历，找出OSD</p>
<p>choose是从bucket中找出若干个type类型的项</p>
<p>chooseleaf操作是bucket中选出若干个type类型的leaf节点</p>
<blockquote>
<p>如果在一台机器上部署3各OSD，这里就要将type后面的“host”改为“OSD”，否则一直是HEALTH_ERR，因为选择规则是在bucket（host）中选择3个host类型的leaf节点，显然你这里没有，改为OSD的话就刚好。</p>
</blockquote>
<p>firstn后面的数字，表示按照前面规则选出节点的个数。如果是0，就按照副本数选择，如果是正数，就按个数来，如果是负数，就按副本数＋负数得到的值来选。</p>
<p>step emit：它首先弹出当前值，并清空栈。它会被典型的应用于rule结尾，也可以用于组织同一条rule的不同树。</p>
<h2 id="Bucket的类型"><a href="#Bucket的类型" class="headerlink" title="Bucket的类型"></a>Bucket的类型</h2><p>Cluster map由device和bucket组成，它们都有id和权重值。Bucket可以包含任意数量item。item可以都是的devices或者都是buckets。管理员控制存储设备的权重。权重和存储设备的容量有关。Bucket的权重被定义为它所包含所有item的权重之和。CRUSH基于4种不同的bucket type，每种有不同的选择算法。</p>
<p>CRUSH映射算法解决了效率和扩展性这两个矛盾的目标。而且当存储集群发生变化时，可以最小化数据迁移，并重新恢复平衡分布。CRUSH定义了四种具有不同算法的的buckets。每种bucket基于不同的数据结构，并有不同的c(r,x)伪随机选择函数。</p>
<p>不同的bucket有不同的性能和特性：</p>
<ul>
<li>Uniform：适用于具有相同权重的item，而且bucket很少添加删除item。它的查找速度是最快的。</li>
<li>List：它的结构是链表结构，所包含的item可以具有任意的权重。CRUSH从表头开始查找副本的位置，它先得到表头item的权重Wh、剩余链表中所有item的权重之和Ws，然后根据hash(x, r, item)得到一个[0~1]的值v，假如这个值v在[0~Wh/Ws)之中，则副本在表头item中，并返回表头item的id。否者继续遍历剩余的链表。</li>
<li>Tree：链表的查找复杂度是O(n)，决策树的查找复杂度是O(log n)。item是决策树的叶子节点，决策树中的其他节点知道它左右子树的权重，节点的权重等于左右子树的权重之和。CRUSH从root节点开始查找副本的位置，它先得到节点的左子树的权重Wl，得到节点的权重Wn，然后根据hash(x, r, node_id)得到一个[0~1]的值v，假如这个值v在[0~Wl/Wn)中，则副本在左子树中，否者在右子树中。继续遍历节点，直到到达叶子节点。Tree Bucket的关键是当添加删除叶子节点时，决策树中的其他节点的node_id不变。决策树中节点的node_id的标识是根据对二叉树的中序遍历来决定的(node_id不等于item的id，也不等于节点的权重)。</li>
<li>Straw：这种类型让bucket所包含的所有item<strong>公平竞争</strong>(不像list和tree一样需要遍历)。这种算法就像抽签一样，所有的item都有机会被抽中(只有最长的签才能被抽中)。每个签的长度是由length = f(Wi)<em>hash(x, r, i) 决定的，f(Wi)和item的权重有关，i是item的id号。c(r, x) = MAXi(f(Wi) </em> hash(x, r, i))。</li>
<li><strong>Straw2</strong>：它是改进的straw bucket。它会在项目A和B的<strong>权重都没有改变时避免任何数据移动</strong>。例如，增加或删除一个项目C，改变它的权重，数据只会移动到它上面或者从它上面移动到其他地方，而不会在bucket内的其他项目之间出现数据移动。因此，该算法减少了集群发生改变后的数据移动量。这是目前使用最广的bucket类型。（《ceph cookbook》）</li>
</ul>
<h2 id="打赏通道"><a href="#打赏通道" class="headerlink" title="打赏通道"></a>打赏通道</h2><p align="center">如果本文对你有所帮助，欢迎小额赞助。您的打赏也是我坚持随笔总结的动力！</p><br><div style="text-align: center"><img src="http://ojet8bprn.bkt.clouddn.com/%E6%89%93%E8%B5%8F1.jpg"></div><br><p align="center">谢谢O(∩_∩)O~</p> 


]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Ceph中严重数据不一致性BUG]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/20/Ceph%E4%B8%AD%E4%B8%A5%E9%87%8D%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%E6%80%A7BUG/</url>
      <content type="html"><![CDATA[<p>目前，块存储服务是Ceph存储中被使用的最普遍的服务之一，通过块存储服务，可以向客户端以使用块设备一样访问Ceph集群。然而，目前在使用块存储服务时，尤其是OpenStack与Ceph对接时，如果没有严格的控制Ceph端的对象大小（使用 &gt;= 8MB对象时），将有可能导致严重的数据不一致情况，该异常由于XFS文件系统本身对Fiemap的支持特性导致的。<br><a id="more"></a></p>
<h2 id="触发场景"><a href="#触发场景" class="headerlink" title="触发场景"></a>触发场景</h2><p>XFS使用fiemap时，当extents数量大于1364时，通过ioctl的FS_IOC_FIEMAP接口，获取的extents数量上限为1364，导致超出部分extents数据获取不到。这将导致，在开启fiemap时，Ceph集群进行recovery与backfill之后，产生大量数据不一致的情况，也是迄今为止Ceph中影响最大的，最严重的数据不一致BUG。</p>
<p>由于fiemap BUG，碎片化对象在recovery与backfill之后（fiemap 获取不正确的数据），从而使的恢复的对象数据与原对象数据不一致。而恢复后的副本若成为主副本，则可能发生静默读错误，并且如果使用Ceph策略自动修复对象（repair object之后），可能将错误数据覆盖至正确数据，数据将永远损毁。在某些情况下，还会触发对象永远处于inconsistent状态。</p>
<h2 id="复现方式"><a href="#复现方式" class="headerlink" title="复现方式"></a>复现方式</h2><p>初始化大量extents碎片文件test，使其成为一个拥有3999个extents的碎片文件。<br><img src="https://www.ustack.com/wp-content/uploads/2016/11/1.png" alt="1"></p>
<p>通过fiemap系统调用，获取这些extents<br><a href="https://www.ustack.com/wp-content/uploads/2016/11/2-1.jpg" target="_blank" rel="external"><img src="https://www.ustack.com/wp-content/uploads/2016/11/2-1.jpg" alt="2"></a><br>编译g++ do_fiemap.cc -o do_fiemap</p>
<p>通过该程序可以打印出可以获取的到fiemap extents, do_fiemap test，通过xfs_bmap test 打印出真正的extents数量后，进行比较。<br><img src="https://www.ustack.com/wp-content/uploads/2016/11/3.jpg" alt="3"></p>
<p>可以看到，我们最先写了2000个extents，并通过xfs_bmap获取到了 2000个有内容的extents，但是通过fiemap系统调用，只获取到了1364个有数据的extents，所以在ceph中使用fiemap系统调用在某些情况下导致数据一致性BUG。</p>
<h2 id="修复方案"><a href="#修复方案" class="headerlink" title="修复方案"></a>修复方案</h2><p>1）目前，在使用块存储使用场景时，通常情况下，默认使用4MB对象。在无特殊情况下，不用使用大于4MB 对象的RBD 镜像。</p>
<p>2）在I版以后，通过新的系统调用，seek_data, seek_hole，可以避免触发该BUG，防止extents过多时使用fiemap调用产生的问题<br><img src="https://www.ustack.com/wp-content/uploads/2016/11/4.png" alt="4"></p>
<p>这组系统调用，可以让使用者通过while循环，反复的发现文件中的data 与 hole，从而组织出一个文件中真正存在的数据，能够避免产生与fiemap系统调用类似的BUG，又保证了recovery或者clone时候只复制有用数据，而反复的系统调，应该会带来一定性能上的影响，请读者们自行测试。</p>
<p>目前，在I版本以后的ceph中，提供参数filestore_seek_data_hole，来启用该功能。当filestore_seek_data_hole 与 filestore_fiemap同时设置时，只用文件系统支持seek_data_hole，那么就会先通过seek_data_hole方式来获取文件的extents，所以在I版本以后的ceph中，应启用filestore_seek_data_hole功能来替代filestore_fiemap 功能。</p>
<p>3）对于已经使用8MB，16MB 甚至更大对象大小的RBD镜像，请暂时设置禁用Fiemap功能，并等待后续版本修复。</p>
<p>原文链接：<a href="https://www.ustack.com/blog/%E9%98%B2%E7%81%AB%E9%98%B2%E7%9B%97%E9%98%B2bug-%E6%9C%89%E4%BA%91%E5%AD%98%E5%82%A8%E5%9B%A2%E9%98%9F%E5%85%AC%E5%B8%83ceph%E4%B8%AD%E6%9C%80%E4%B8%A5%E9%87%8D%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80/" target="_blank" rel="external">有云存储团队公布Ceph中最严重数据不一致BUG!</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[块存储的世界]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/20/%E5%9D%97%E5%AD%98%E5%82%A8%E7%9A%84%E4%B8%96%E7%95%8C/</url>
      <content type="html"><![CDATA[<p>在OpenStack中，存储是非常重要的一块，但是因为其提供方式的不同，即使是专业的工程师也会感到十分困惑。OpenStack的存储主要分为三大类，一是对象    存储服务，Swift解决的问题，二是块设备存储服务，在OpenStack里主要是提供给虚拟机的作为“硬盘”的存储，这里又分为本地块存储和分布式块存储，Cinder项目正    在解决相关的问题；第三类，数据库存储服务，Databases as a Service(DBaaS), 对应AWS里面的RDC，目前是一个正在孵化的项目，Trove，前身是Rackspace开源出&gt;    来的RedDwarf。本文重点介绍块存储服务。<br><a id="more"></a></p>
<blockquote>
<p>转载的文章，原文排版看的心累，链接见文末。</p>
</blockquote>
<p>​    UnitedStack OS的块存储是在OpenStack Cinder框架下开发的，开发过程中，我们对目前主流的块存储服务提供商和开源的块存储软件做了一个简要分析，希望能给从事块存储开发的工程师对于块存储一个全局的认识。        </p>
<p>​    块存储，简单来说就是提供了块设备存储的接口。用户需要把块存储卷附加到虚拟机(或者裸机)上后才可以与其交互。这些卷都是持久的: 它们可以被从运行实例上解除或者重新附加而数据保持完整不变。下面会先介绍常见的单机块设备工具来建立对块存储的初步印象。</p>
<h2 id="单机块存储"><a href="#单机块存储" class="headerlink" title="单机块存储"></a>单机块存储</h2><p>​    首先一个硬盘是一个块设备，内核检测到硬盘然后在/dev/下会看到/dev/sda/。因为我们需要利用一个硬盘来得到不同的分区来做不同的事，通过fdisk工具得到/dev/sda1, /dev/sda2等，这种通过直接写入分区表来规定和切分硬盘,是最死板的分区方式。</p>
<h3 id="LVM-amp-Device-mapper"><a href="#LVM-amp-Device-mapper" class="headerlink" title="LVM &amp; Device-mapper"></a>LVM &amp; Device-mapper</h3><p>​    LVM是一种逻辑卷管理器，通过LVM来对硬盘创建逻辑卷组和得到逻辑卷来完成目的比fdisk方式更加弹性。LVM对于不太熟悉其机制的人看来可能会造成不小的开销，但是相对于LVM对来的易用性其映射机制的负载小的足以忽略(在snapshot的情况会有不小的负载)。</p>
<p>​    LVM在大多数Linux使用者看来非常普通的工具，它同时也是OpenStack块存储项目的一种backend并且是默认的。通过LVM在每个主机创建Volume Group，通过Cinder(OpenStack块存储项目)的调度器可以在任何一个合适的主机(满足容量和其他要求)使用lvcreate创建一个块设备供Nova(OpenStack Computing项目)使用。</p>
<p>​    Device-mapper是一种支持逻辑卷管理的通用设备映射机制，为存储资源管理的块设备驱动提供了一个高度模块化的内核架构。LVM是基于Device-mapper的用户程序实现。</p>
<p>​    Device-mapper在内核中它通过一个模块化的Target Driver插件实现对IO请求的过滤或者重定向工作，当前已经实现的Target Driver插件包括软Raid、软加密、逻辑卷条带、多路径、镜像、快照等。整个 device mapper 机制由两部分组成–内核空间的 device mapper 驱动、用户空间的device mapper 库以及它提供的 dmsetup 工具。内核中主要提供完成这些策略所需要的机制。Device-mapper 用户空间相关部分主要负责配置具体的策略和控制逻辑，比如逻辑设备和哪些物理设备建立映射，怎么建立这些映射关系等等，而具体过滤和重定向 IO 请求的工作由内核中相关代码完成。</p>
<h3 id="SAN-amp-iSCSI"><a href="#SAN-amp-iSCSI" class="headerlink" title="SAN &amp; iSCSI"></a>SAN &amp; iSCSI</h3><p>​    在接触了单机下的逻辑卷管理后，你需要了解<a href="http://zh.wikipedia.org/wiki/%E5%AD%98%E5%82%A8%E5%8C%BA%E5%9F%9F%E7%BD%91%E7%BB%9C" target="_blank" rel="external">SAN</a>，目前主流的企业级存储方式。</p>
<p>​    大部分SAN使用<a href="http://en.wikipedia.org/wiki/SCSI" target="_blank" rel="external">SCSI</a>协议在服务器和存储设备之间传输和沟通，通过在SCSI之上建立不同镜像层，可以实现存储网络的连接。常见的有<a href="http://zh.wikipedia.org/wiki/ISCSI" target="_blank" rel="external">iSCSI</a>，<a href="http://en.wikipedia.org/wiki/Fibre_Channel_Protocol" target="_blank" rel="external">FCP</a>，<a href="http://en.wikipedia.org/wiki/Fibre_Channel_over_Ethernet" target="_blank" rel="external">Fibre Channel over Ethernet</a>等。</p>
<p>​    SAN通常需要在专用存储设备中建立，而iSCSI是基于TCP/IP的SCSI映射，通过iSCSI协议和<a href="http://linux-iscsi.sourceforge.net/" target="_blank" rel="external">Linux iSCSI</a>项目我们可以在常见的PC机上建立SAN存储。。</p>
<p>​    如何建立在PC机上的SAN可以参考<a href="http://linux.vbird.org/linux_server/0460iscsi.php" target="_blank" rel="external">iSCSI建立</a>，这篇文章的iSCSI target管理方式不太方便，通常利用<a href="http://linux-iscsi.org/wiki/Targetcli" target="_blank" rel="external">targetcli</a>管理target是及其方便的。targetcli可以直接建立和管理不同backstone类型的逻辑卷和不同的export方式，如建立ramdisk并且通过iSCSI export非常方便，操作方式见<a href="http://www.youtube.com/watch?v=mKjBsgOlYmE" target="_blank" rel="external">targetcli screencast Part 2 of 3: ISCSI – YouTube</a>。</p>
<h2 id="分布式块存储服务"><a href="#分布式块存储服务" class="headerlink" title="分布式块存储服务"></a>分布式块存储服务</h2><p>​    以上都是我们经常解除的单机下块存储环境，接下来开始本文的正式分享内容，包括公共云技术服务提供的块存储服务，开源的块存储框架和OpenStack目前对块存储的定义和支持情况。</p>
<h3 id="分布式块存储"><a href="#分布式块存储" class="headerlink" title="分布式块存储"></a>分布式块存储</h3><p>​    在面对极具弹性的存储需求和性能要求下，单机或者独立的SAN越来越不能满足企业的需要。如同数据库系统一样，块存储在scale up的瓶颈下也面临着scale out的需要。我们可以用以下几个特点来描述分布式块存储系统的概念:</p>
<ul>
<li>分布式块存储可以为任何物理机或者虚拟机提供持久化的块存储设备</li>
<li>分布式块存储系统管理块设备的创建、删除和attach/deattach。</li>
<li>分布式块存储支持强大的快照功能，快照可以用来恢复或者创建新的块设备</li>
<li>分布式存储系统能够提供不同IO性能要求的块设备</li>
</ul>
<h3 id="Amazon-EBS-amp-阿里云磁盘"><a href="#Amazon-EBS-amp-阿里云磁盘" class="headerlink" title="Amazon EBS &amp; 阿里云磁盘"></a>Amazon EBS &amp; 阿里云磁盘</h3><p>​    Amazon作为领先的IAAS服务商，其API目前是IAAS的事实标准。Amazon EC2目前仍然一骑绝尘，在大多数方面远超其他IAAS服务商。通过Amazon EC2的<a href="http://aws.amazon.com/cn/ec2/" target="_blank" rel="external">产品介绍</a>是快速了解Amazon EC2的捷径。</p>
<p>​    而<a href="http://aws.amazon.com/cn/ebs/" target="_blank" rel="external">EBS</a>是Amazon提供的块存储服务，通过EBS，用户可以随时增删迁移volume和快照操作。</p>
<p>​    Amazon EC2实例可以将根设备数据存储在Amazon EBS或者本地实例存储上。使用 Amazon EBS时，根设备中的数据将独立于实例的生命周期保留下来，使得在停止实例后仍可以重新启动使用，与笔记本电脑关机并在再次需要时重新启动相似。另一方面，本地实例存储仅在实例的生命周期内保留。这是启动实例的一种经济方式，因为数据没有存储到根设备中。</p>
<p>​    Amazon EBS提供两种类型的卷，即标准卷和预配置IOPS卷。它们的性能特点和价格不同，可以根据应用程序的要求和预算定制所需的存储性能。</p>
<p>​    标准卷可为要求有适度或突发式I/O的应用程序提供存储。这些卷平均可以提供大约 100 IOPS，最多可突增至数百 IOPS。标准卷也非常适合用作引导卷，其突发能力可提供快速的实例启动时间(通常十几秒)。</p>
<p>​    预配置 IOPS 卷旨在为数据库等 I/O 密集型随机读写工作负载提供可预计的高性能。创建一个卷时，利用预置 IOPS 为卷确定 IOPS 速率，随之 Amazon EBS 在该卷的生命周期内提供该速率。Amazon EBS 目前支持每预配置 IOPS 卷最多 4000 个IOPS。您可以将多个条带式卷组合在一起，为您的应程程序提供每个Amazon EC2数千IOPS的能力。</p>
<p>​    EBS可以在卷连接和使用期间实时拍摄快照。不过，快照只能捕获已写入Amazon EBS 卷的数据，不包含应用程序或操作系统已在本地缓存的数据。如果需要确保能为实例连接的卷获得一致的快照，需要先彻底地断开卷连接，再发出快照命令，然后重新连接卷。</p>
<p>​    EBS快照目前可以跨regions增量备份，意味着EBS快照时间会大大缩短，从另一面增加了EBS使用的安全性。</p>
<p>​    总的来说，Amazon EBS是目前IAAS服务商最引入注目的服务之一，目前的OpenStack、CloudStack等等其他开源框架都无法提供Amazon EBS对于的如此弹性和强大的服务。了解和使用Amazon EBS是学习IAAS块存储的最好手段。</p>
<h3 id="阿里云"><a href="#阿里云" class="headerlink" title="阿里云"></a>阿里云</h3><p>​    阿里云是国内的公共云计算服务商，不过这里阿里云目前的块存储服务较于Amazon EBS差的太远，阿里云磁盘目前仅支持在创建云主机的时候绑定云磁盘或者在升级云主机的进行云磁盘扩容，这从根本上就是传统的虚拟主机的特点而不是所谓的“云磁盘”。</p>
<p>从目前的阿里云磁盘的限制:</p>
<ul>
<li>无法快速创建或删除volume，在进行扩容时需要升级云主机才能达到，而升级云主机只有在下月云主机套餐到期时才能生效(想起了中国移动套餐)</li>
<li>一个云主机最多只能绑定3个云磁盘</li>
</ul>
<p>从阿里云磁盘目前的使用分析，阿里云磁盘系统目前还很不成熟，以下是我对阿里云磁盘实现的推测</p>
<ul>
<li>阿里云主机是跟磁盘绑定的，这意味着阿里云的云磁盘是local volume(因此性能还是挺可观的)。如果用户需要扩容、减少都是需要下个月更说明了这点，整个主机在扩容时去调度合适的有足够存储空间的host，然后进行扩容。</li>
<li>阿里云磁盘是分布式块存储系统，但是由于其QoS无法保证和其他资源调度原因无法提供足够的块存储支持。</li>
</ul>
<p>从<a href="http://www.infoq.com/cn/news/2013/06/aliyun-nihao-storage" target="_blank" rel="external">演讲回顾：阿里云存储技术的演进，以及云服务用例最佳实践</a>中了解到阿里云是基于自家的“盘古”系统，那么从实际使用来说，远没达到一般的分布式块存储系统的要求。</p>
<h3 id="Ceph-amp-Sheepdog"><a href="#Ceph-amp-Sheepdog" class="headerlink" title="Ceph &amp; Sheepdog"></a>Ceph &amp; Sheepdog</h3><p><a href="http://ceph.com/" target="_blank" rel="external">Ceph</a>是开源实现的PB级分布式文件系统，通过其分布式对象存储机制为上层提供了文件接口、块存储接口和对象存储接口。<a href="http://www.inktank.com/" target="_blank" rel="external">Inktank</a>是Ceph的主要支持商，也是目前Ceph开源社区的主要力量。</p>
<p><img src="https://www.ustack.com/wp-content/themes/officalsite/uploads/2013/07/11.png" alt="1"></p>
<p>​    Ceph目前是OpenStack支持的Backend中一个不错的开源块存储实现系统(即Cinder项目backend driver之一)，其实现分为三个部分: OSD, Monitor, MDS。OSD是底层对象存储系统，Monitor是集群管理系统，MDS是用来支持POSIX文件接口的Metadata Server。从Ceph的原始论文(<a href="http://ceph.newdream.net/papers/weil-thesis.pdf" target="_blank" rel="external">Ceph: Reliable, Scalable, and High-Performance Distributed Storage</a>)来看，Ceph专注于扩展性，高可用性和容错性。Ceph放弃了传统的Metadata查表方式(HDFS)而改用算法(CRUSH)去定位具体的block。</p>
<p>​    利用Ceph提供的RULES可以弹性地制订存储策略和Pool选择，Monitor作为集群管理系统掌握了全部的Cluster Map，Client在没有Map的情况下需要先向Monitor请求得到，然后通过Object id计算相应的OSD Server。</p>
<p>​    Ceph支持传统的POSIX文件接口，因此需要额外的MDS(Meatadata Server)支持文件元信息(Ceph的块存储和对象存储支持不需要MDS服务)。Ceph将Data和Metadata分离到两个服务上，跟传统的分布式系统如Lustre相比可以大大增强扩展性。在小文件读写上，Ceph读写文件会有[RTT*2]，在每次open时，会先去Metadata Server查询一次，然后再去Object Server。除了Open操作外，Ceph在Delete上也有问题，它需要到Metadata Server擦除对应的Metadata，是n(2)复杂度。Ceph在Metadata上并非只有坏处，通过Metadata Server，像目录列表等目录操作为非常快速，远超GlusterFS等其他分布式文件系统的目录或文件元操作。</p>
<p>利用Ceph可以作两种不同类型的“共享存储”支持:</p>
<ol>
<li>利用CephFS作分布式系统，所有虚拟机的块设备直接使用CephFS的命名空间</li>
<li>使用Ceph RBD块设备接口，从Ceph Cluster中导出Volume作为块设备Attach到虚拟机。</li>
</ol>
<p>关于Ceph作为块存储项目的几个问题需要考虑:</p>
<ul>
<li>Ceph在读写上不太稳定（有Btrfs的原因），目前Ceph官方推荐XFS作为底层文件系统</li>
<li>Ceph的扩展性和架构较复杂，如果需要介入Ceph，需要较长时间深入了解</li>
<li>Ceph的部署不够简易，目前Ceph的官方部署工具不太成熟，而对于的Pupet模块或者其他都没有公认的较好实现。</li>
<li>Ceph的稳定性依赖高版本Linux的一些特性，并且Ceph的rbd.ko在高版本中更具稳定性。</li>
</ul>
<h3 id="Sheepdog"><a href="#Sheepdog" class="headerlink" title="Sheepdog"></a>Sheepdog</h3><p>​    <a href="http://www.osrg.net/sheepdog/" target="_blank" rel="external">Sheepdog</a>是另一个分布式块存储系统实现，它与Ceph相比，最大优势就是代码短小好维护和hack的成本很小。Sheepdog也有很多Ceph不支持的特性，比如说Multi-Disk, Cluster-wide Snapshot等。</p>
<p>​    Sheepdog主要有两部分，一个是集群管理，另一个是存储服务。集群管理目前使用Corosync或者Zookper来完成，其存储服务的特点是在Client和存储Host有Cache的实现可以大大减小数据流量。</p>
<p>​    目前Sheepdog只在QEMU端提供Drive，而缺少library支持，这是Sheepdog目前最主要的问题。但是社区已经有相关的Blueprint在讨论这个问题。</p>
<p>了解Sheepdog通过以下链接:</p>
<ul>
<li><a href="http://www.slideshare.net/multics/overview-of-sheepdog" target="_blank" rel="external">Sheepdog Overview</a></li>
<li><a href="http://rdc.taobao.com/blog/cs/?tag=sheepdog" target="_blank" rel="external">Sheepdog 淘宝核心系统团队</a></li>
<li><a href="https://github.com/collie/sheepdog/wiki" target="_blank" rel="external">Sheepdog wiki</a>: Sheepdog的一系列Wiki如同它的代码一样简短出色</li>
</ul>
<p>目前Taobao是Sheepdog主要用户和社区贡献者，国内也有Startup参与Sheepdog的社区开发。</p>
<h3 id="Cinder"><a href="#Cinder" class="headerlink" title="Cinder"></a>Cinder</h3><p>​    <a href="https://wiki.openstack.org/wiki/Main_Page" target="_blank" rel="external">OpenStack</a>是目前流行的IAAS框架，提供了AWS类似的服务并且兼容其API。OpenStack Nova是计算服务，Swift是对象存储服务，Quantum是网络服务，Glance是镜像服务，Cinder是块存储服务，Keystone是身份认证服务，Horizon是Dashboard，另外还有Heat、Oslo、Ceilometer、Ironic等等项目。</p>
<p>​    <a href="https://wiki.openstack.org/wiki/Cinder" target="_blank" rel="external">Cinder</a>是OpenStack中提供类似于EBS块存储服务的API框架，它并没有实现对块设备的管理和实际服务提供，用来为后端不同的存储结构提供统一的接口与OpenStack进行整合，不同的块设备服务厂商在Cinder中实现其驱动支持。后端的存储可以是DAS，NAS，SAN，对象存储或者分布式文件系统。也就是说，Cinder的块存储数据完整性，可用性保障是由后端存储提供的。在<a href="https://wiki.openstack.org/wiki/CinderSupportMatrix" target="_blank" rel="external">CinderSupportMatrix</a>中可以看到众多存储厂商如NetAPP、IBM、SolidFire、EMC和众多开源块存储系统对Cinder的支持，在这里我们也可以看到OpenStack是非常受欢迎的。</p>
<p><img src="https://www.ustack.com/wp-content/themes/officalsite/uploads/2013/07/2.png" alt="2"></p>
<p>​    从上图我们也可以看到，Cinder只是提供了一层抽象，然后通过其后段支持的driver实现来发出命令来得到回应。关于块存储的分配信息以及选项配置等会被保存到OpenStack统一的DB中。</p>
<p>​    目前Cinder项目支持的操作包括创建/删除Volume，创建/删除Snapshot，Clone Volume，将Volume制作成Image或者将Image导出到Volume，备份/恢复Volume和扩展Volume大小，不同的Backend会实现不同程度的操作，但基本的创建Volume和快照操作都是支持的。目前令人振奋的Amazon EBS的QoS机制已经在Cinder社区中得到广泛讨论，在经历近两个月的Patch Review之后，整个QoS机制即将正式进入Cinder项目。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    目前分布式块存储的实现仍然是由Amazon EBS领衔，其卓越稳定的读写性能、强大的增量快照和跨区域块设备迁移，以及令人惊叹的QoS控制都是目前开源或者其他商业实现无法比拟的。</p>
<p>​    不过Amazon EBS始终不是公司私有存储的一部分，作为企业IT成本的重要部分，块存储正在发生改变。EMC在一个月前发布了其ViPR平台，并开放了其接口试图接纳其他厂商和开源实现。Nexenta在颠覆传统的的存储专有硬件，在其上软件实现原来只有专有SDN的能力，让企业客户完全摆脱存储与厂商的绑定。Inktank极力融合OpenStack并推动Ceph在OpenStack社区的影响力都说明了无论是目前的存储厂商还是开源社区都在极力推动整个分布式块存储的发展，存储专有设备的局限性正在进一步弱化了原有企业的存储架构。</p>
<p>​    在分布式块存储和OpenStack之间我们可以打造更巩固的纽带，UnitedStack存储团队在开源存储的基础上，结合本地块存储和分布式块存储的优势，为UnitedStack OS提供一个通用的存储解决方案。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://www.ibm.com/developerworks/cn/linux/filesystem/lvm/lvm-1/" target="_blank" rel="external">通用线程: 学习 Linux LVM，第 1 部分</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/filesystem/lvm/lvm-2/" target="_blank" rel="external">通用线程：学习 Linux LVM，第 2部分</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-devmapper/" target="_blank" rel="external">Linux 内核中的 Device Mapper 机制</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/linux/l-ceph/" target="_blank" rel="external">Ceph：一个 Linux PB 级分布式文件系统</a></li>
<li><a href="http://www.alidata.org/archives/1589" target="_blank" rel="external">分布式文件系统Ceph调研1 – RADOS</a></li>
<li><a href="http://ceph.com/docs/next/architecture/" target="_blank" rel="external">Ceph Architecture</a></li>
<li><a href="http://way4ever.com/?p=375" target="_blank" rel="external">Ceph的现状</a></li>
<li><a href="http://way4ever.com/?p=122" target="_blank" rel="external">ceph的CRUSH数据分布算法介绍</a></li>
<li><a href="http://ceph.com/docs/next/dev/" target="_blank" rel="external">Ceph INTERNAL DEVELOPER DOCUMENTATION</a></li>
<li>原文链接：<a href="https://www.ustack.com/blog/block-storage-overview/" target="_blank" rel="external">块存储的世界</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux命令——uptime]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/17/Linux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94uptime/</url>
      <content type="html"><![CDATA[<p>打印系统总共运行了多长时间和系统的平均负载。<br><a id="more"></a></p>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Usage:</div><div class="line"> uptime [options]</div><div class="line"></div><div class="line">Options:</div><div class="line"> -p, --pretty   show uptime in pretty format</div><div class="line"> -h, --help     display this help and exit</div><div class="line"> -s, --since    system up since</div><div class="line"> -V, --version  output version information and exit</div><div class="line"></div><div class="line">For more details see uptime(1).</div></pre></td></tr></table></figure>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><h3 id="uptime"><a href="#uptime" class="headerlink" title="uptime"></a>uptime</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># uptime</div><div class="line"> 17:40:45 up 1 day,  6:37,  3 users,  load average: 0.27, 0.21, 0.31</div></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>显示内容</th>
<th>释义</th>
</tr>
</thead>
<tbody>
<tr>
<td>17:40:45</td>
<td>系统当前时间</td>
</tr>
<tr>
<td>up 1 day,  6:37</td>
<td>主机已运行时间，时间越大，说明机器越稳定</td>
</tr>
<tr>
<td>3 users</td>
<td>用户连接数，是总连接数而不是用户数</td>
</tr>
<tr>
<td>load average: 0.27, 0.21, 0.31</td>
<td>最近1，5，15分钟的系统平均负载</td>
</tr>
</tbody>
</table>
<p><strong>系统平均负载</strong>：在特定时间间隔内运行队列中的平均进程数。</p>
<p><strong>查看CPU内核数</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># cat /proc/cpuinfo |grep &quot;processor&quot;|wc -l</div><div class="line">4</div></pre></td></tr></table></figure>
<p>如果每个CPU内核的当前活动进程数不大于3的话，那么系统的性能是良好的。如果每个CPU内核的任务数大于5，那么这台机器的性能有严重问题。</p>
<h3 id="运行时间格式输出"><a href="#运行时间格式输出" class="headerlink" title="运行时间格式输出"></a>运行时间格式输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># uptime -p</div><div class="line">up 1 day, 6 hours, 56 minutes</div></pre></td></tr></table></figure>
<h3 id="系统启动的时间"><a href="#系统启动的时间" class="headerlink" title="系统启动的时间"></a>系统启动的时间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># uptime -s</div><div class="line">2017-01-16 11:03:35</div></pre></td></tr></table></figure>
<h3 id="uptime版本"><a href="#uptime版本" class="headerlink" title="uptime版本"></a>uptime版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># uptime -V</div><div class="line">uptime from procps-ng 3.3.9</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[dd磁盘命令小记]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/12/dd%E7%A3%81%E7%9B%98%E5%91%BD%E4%BB%A4%E5%B0%8F%E8%AE%B0/</url>
      <content type="html"><![CDATA[<p>dd是经常使用的命令了，方便快捷，这里闲来小总结一下，以及怎样测出最真实的写入速度。<br><a id="more"></a></p>
<h2 id="命令的使用"><a href="#命令的使用" class="headerlink" title="命令的使用"></a>命令的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># dd --help</div><div class="line">Usage: dd [OPERAND]...</div><div class="line">  or:  dd OPTION</div><div class="line">Copy a file, converting and formatting according to the operands.</div><div class="line"></div><div class="line">  bs=BYTES        read and write up to BYTES bytes at a time</div><div class="line">  cbs=BYTES       convert BYTES bytes at a time</div><div class="line">  conv=CONVS      convert the file as per the comma separated symbol list</div><div class="line">  count=N         copy only N input blocks</div><div class="line">  ibs=BYTES       read up to BYTES bytes at a time (default: 512)</div><div class="line">  if=FILE         read from FILE instead of stdin</div><div class="line">  iflag=FLAGS     read as per the comma separated symbol list</div><div class="line">  obs=BYTES       write BYTES bytes at a time (default: 512)</div><div class="line">  of=FILE         write to FILE instead of stdout</div><div class="line">  oflag=FLAGS     write as per the comma separated symbol list</div><div class="line">  seek=N          skip N obs-sized blocks at start of output</div><div class="line">  skip=N          skip N ibs-sized blocks at start of input</div><div class="line">  status=WHICH    WHICH info to suppress outputting to stderr;</div><div class="line">                  &apos;noxfer&apos; suppresses transfer stats, &apos;none&apos; suppresses all</div></pre></td></tr></table></figure>
<p><strong>翻译</strong></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>bs</td>
<td>同时设置读写块的大小为 bytes ，可代替 ibs 和 obs</td>
</tr>
<tr>
<td>cbs</td>
<td>一次转换 bytes 个字节，即转换缓冲区大小</td>
</tr>
<tr>
<td>conv</td>
<td>转换参数</td>
</tr>
<tr>
<td>count</td>
<td>复制的块数</td>
</tr>
<tr>
<td>ibs</td>
<td>一次读入 bytes 个字节(即一个块大小为 bytes 个字节)</td>
</tr>
<tr>
<td>if</td>
<td>输入文件 或 设备名称</td>
</tr>
<tr>
<td>iflag</td>
<td>按照逗号来分隔读参数</td>
</tr>
<tr>
<td>obs</td>
<td>一次写 bytes 个字节(即一个块大小为 bytes 个字节)</td>
</tr>
<tr>
<td>of</td>
<td>输出文件 或 设备名称</td>
</tr>
<tr>
<td>oflag</td>
<td>按照逗号来分隔写参数</td>
</tr>
<tr>
<td>seek</td>
<td>从输出文件开头跳过 blocks 个块后再开始复制</td>
</tr>
<tr>
<td>skip</td>
<td>从输入文件开头跳过 blocks 个块后再开始复制</td>
</tr>
</tbody>
</table>
<p>查看版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dd --version</div></pre></td></tr></table></figure>
<h2 id="磁盘读写"><a href="#磁盘读写" class="headerlink" title="磁盘读写"></a>磁盘读写</h2><p><strong>两个特殊的设备</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">/dev/null：伪设备，相当于黑洞，of到该设备不会产生IO</div><div class="line">/dev/zero：伪设备，它只产生空字符流，对它不会产生IO</div></pre></td></tr></table></figure>
<p><strong>1、测试磁盘写能力</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dd if=/dev/zero of=/test.a bs=8k count=10000</div></pre></td></tr></table></figure>
<p>IO都集中在of文件中，of文件只用于写，相当于测试磁盘的写能力。</p>
<p><strong>2、测试磁盘读能力</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dd if=/dev/sdb1 of=/dev/null bs=8k count=10000</div></pre></td></tr></table></figure>
<p>IO只发生在/dev/sdb1上，相当于测试磁盘的读能力。</p>
<p><strong>3、测试同时读写能力</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dd if=/dev/sdb1 of=/test.a bs=8k count=10000</div></pre></td></tr></table></figure>
<p>一个是物理分区，一个是实际的文件，对它们的读写都会产生IO，如果它们都在一个磁盘中，该命令就相当于测试磁盘的同时读写能力。</p>
<h2 id="几种测试写磁盘的区别"><a href="#几种测试写磁盘的区别" class="headerlink" title="几种测试写磁盘的区别"></a>几种测试写磁盘的区别</h2><p>先给出四种测试方式的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># dd if=/dev/zero of=test bs=1M count=128</div><div class="line">128+0 records in</div><div class="line">128+0 records out</div><div class="line">134217728 bytes (134 MB) copied, 0.116559 s, 1.2 GB/s</div><div class="line"></div><div class="line"># dd if=/dev/zero of=test bs=1M count=128 ; sync</div><div class="line">128+0 records in</div><div class="line">128+0 records out</div><div class="line">134217728 bytes (134 MB) copied, 0.125106 s, 1.1 GB/s</div><div class="line"></div><div class="line"># dd if=/dev/zero of=test bs=1M count=128 conv=fdatasync</div><div class="line">128+0 records in</div><div class="line">128+0 records out</div><div class="line">134217728 bytes (134 MB) copied, 1.76802 s, 75.9 MB/s</div><div class="line"></div><div class="line"># dd if=/dev/zero of=test bs=1M count=128 oflag=dsync</div><div class="line">128+0 records in</div><div class="line">128+0 records out</div><div class="line">134217728 bytes (134 MB) copied, 4.91353 s, 27.3 MB/s</div></pre></td></tr></table></figure>
<p>1、<code>dd if=/dev/zero of=test bs=1M count=128</code></p>
<p>没有加任何参数，命令只是单纯地把这128MB的数据读到内存缓冲当中，dd命令完成前并没有让系统真正把文件写到磁盘上。所以得到的是一个超级快的速度。直到dd完成后系统才开始真正往磁盘上写数据,但这个速度你是看不到了。</p>
<p>2、<code>dd if=/dev/zero of=test bs=1M count=128 ; sync</code></p>
<p>这事先后两个独立的命令。当sync命令准备开始往磁盘上真正写入数据的时候，前面dd命令已经把错误的“写入速度”值显示在屏幕上了。所以得到的不是真正的写入速度。</p>
<p><strong>3、<code>dd if=/dev/zero of=test bs=1M count=128 conv=fdatasync</code></strong></p>
<p>加入参数 <code>conv=fdatasync</code> 后，dd命令执行到最后会真正执行一次“同步(sync)”操作，所以这时候得到的是读取这128M数据到内存并写入到磁盘上所需的时间，这样算出来的时间才是<strong>比较符合实际</strong>的。这种方式最接近计算机实际操作，所以测出来的数据最有参考价值。</p>
<p>4、<code>dd if=/dev/zero of=test bs=1M count=128 oflag=dsync</code></p>
<p>加入参数 <code>oflag=dsync</code> 后，dd在执行时每次都会进行同步写入操作。这条命令每次读取1M后就要先把这1M写入磁盘，然后再读取下面这1M，一共重复128次。这是最慢的一种方式，因为基本上没有用到写缓存(write cache)。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[解决calamari无法获取集群hosts信息的BUG]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/12/%E8%A7%A3%E5%86%B3calamari%E6%97%A0%E6%B3%95%E8%8E%B7%E5%8F%96%E9%9B%86%E7%BE%A4hosts%E4%BF%A1%E6%81%AF%E7%9A%84BUG/</url>
      <content type="html"><![CDATA[<p>如题，环境是ubuntu 14.04，解决过程和方法详见正文。<br><a id="more"></a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前在磨磨的文章中看到这个问题，他是在CentOS上，通过修改 <code>/usr/lib/python2.7/site-packages/salt/master.py</code> 文件，然后重启salt-master和salt-minion服务来解决的，链接在文末。原理类似，可ubuntu上并不能完全照搬。</p>
<h2 id="折腾过程"><a href="#折腾过程" class="headerlink" title="折腾过程"></a>折腾过程</h2><blockquote>
<p>这部分的内容为折腾求证的过程，想看解决方法的可以直接跳过！</p>
</blockquote>
<p>在ubuntu上并没有这个文件和路径，通过tab可以看到site开头的只有这几个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@calamari:~# vim /usr/lib/python2.7/site</div><div class="line">sitecustomize.py   sitecustomize.pyc  site.py            site.pyc</div></pre></td></tr></table></figure>
<p>我以为可能是系统下包安装的位置不对而已，好，那我就查看一下ubuntu下salt-master的安装位置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">root@calamari:~# dpkg -L salt-master</div><div class="line">/.</div><div class="line">/usr</div><div class="line">/usr/share</div><div class="line">/usr/share/man</div><div class="line">/usr/share/man/man1</div><div class="line">/usr/share/man/man1/salt.1.gz</div><div class="line">/usr/share/man/man1/salt-run.1.gz</div><div class="line">/usr/share/man/man1/salt-master.1.gz</div><div class="line">/usr/share/man/man1/salt-key.1.gz</div><div class="line">/usr/share/man/man1/salt-cp.1.gz</div><div class="line">/usr/share/doc</div><div class="line">/usr/share/doc/salt-master</div><div class="line">/usr/share/doc/salt-master/changelog.Debian.gz</div><div class="line">/usr/share/doc/salt-master/NEWS.Debian.gz</div><div class="line">/usr/share/doc/salt-master/copyright</div><div class="line">/usr/bin</div><div class="line">/usr/bin/salt-run</div><div class="line">/usr/bin/salt-master</div><div class="line">/usr/bin/salt-key</div><div class="line">/usr/bin/salt-cp</div><div class="line">/usr/bin/salt</div><div class="line">/lib</div><div class="line">/lib/systemd</div><div class="line">/lib/systemd/system</div><div class="line">/lib/systemd/system/salt-master.service</div><div class="line">/etc</div><div class="line">/etc/init.d</div><div class="line">/etc/init.d/salt-master</div><div class="line">/etc/init</div><div class="line">/etc/init/salt-master.conf</div><div class="line">/etc/salt</div><div class="line">/etc/salt/master</div><div class="line">/etc/salt/master.d</div></pre></td></tr></table></figure>
<p>然后通过 <code>find</code> 命令去 <code>/etc, /usr, /lib</code> 这几个目录下找 <code>master.py</code> ，结果真没有</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">find: `master.py&apos;: No such file or directory</div></pre></td></tr></table></figure>
<p>好吧，下面来看问题。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在calamari的web界面下，查看 MANAGE -&gt; Cluster -&gt;HOSTS 信息，发现啥也没有，如下图所示：<img src="http://ojet8bprn.bkt.clouddn.com/calamari-1.png" alt="calamari-1"></p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>calamari的salt-master节点在读取</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/var/cache/salt/master/minions/&#123;minion-hostname&#125;/data.p</div></pre></td></tr></table></figure>
<p>该文件的时候有权限问题，当前权限为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-rw------- 1 root root 3019 Jan 10 11:06 data.p</div></pre></td></tr></table></figure>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>修改权限为 644 即可，写了个脚本，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line">for x in 0 1 2</div><div class="line">do</div><div class="line">    chmod 644 /var/cache/salt/master/minions/ceph$x/data.p</div><div class="line">done</div></pre></td></tr></table></figure>
<p>修改完后，再次查看，可以看到hosts的详细信息了<img src="http://ojet8bprn.bkt.clouddn.com/calamari-2.png" alt="calamari-2"></p>
<blockquote>
<p>当然，重启 salt-minion 后又需要在 salt-master 机器上重新执行一遍，<del>~~(&gt;_&lt;)</del>~~，忧伤~</p>
</blockquote>
<p>磨磨的方法：<a href="http://www.zphj1987.com/2017/01/09/calamari-node-info/" target="_blank" rel="external">centos下该问题的解决方法</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[ubuntu下配置邮件发送]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/11/ubuntu%E4%B8%8B%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81/</url>
      <content type="html"><![CDATA[<p>Linux下发送email有很多种方式，包括sendmail、mailx、mutt、uuencode等等。这里介绍ubuntu下使用mutt和msmtp发送邮件的配置方法。<br><a id="more"></a></p>
<h2 id="软件包的安装"><a href="#软件包的安装" class="headerlink" title="软件包的安装"></a>软件包的安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install mutt</div><div class="line">sudo apt-get install msmtp</div></pre></td></tr></table></figure>
<h2 id="文件的配置"><a href="#文件的配置" class="headerlink" title="文件的配置"></a>文件的配置</h2><h3 id="配置-mutt"><a href="#配置-mutt" class="headerlink" title="配置 mutt"></a>配置 mutt</h3><p>,系统全局设置配置文件在 /etc/Muttrc,如果使用某个系统用户，可以在~/.muttc中设置，没有该文件，就自己创建。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">vi .muttrc</div><div class="line"></div><div class="line">set sendmail=&quot;/usr/bin/msmtp&quot;</div><div class="line">set use_from=yes</div><div class="line">set realname=&quot;xia&quot;</div><div class="line">set from=abcdefg@126.com</div><div class="line">set envelope_from=yes</div></pre></td></tr></table></figure>
<h3 id="配置msmtp"><a href="#配置msmtp" class="headerlink" title="配置msmtp"></a>配置msmtp</h3><p>创建 ~/.msmtprc 和 ~/.msmtp.log，分别为配置文件和日志文件。</p>
<p><strong>1、创建配置文件</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vi .msmtprc</div></pre></td></tr></table></figure>
<p>添加以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">account default</div><div class="line">host smtp.126.com</div><div class="line">from youremailname@126.com</div><div class="line">auth plain</div><div class="line">user youremailname@126.com</div><div class="line">password xxxxxxx</div><div class="line">logfile ~/.msmtp.log</div></pre></td></tr></table></figure>
<p>由于password是明文，所以需要修改此文件的访问权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">chmod 600 .msmtprc</div></pre></td></tr></table></figure>
<p><strong>特别注意</strong>：</p>
<blockquote>
<p>这里的密码填的是你的授权密码，不是登录密码!</p>
</blockquote>
<p><strong>2、创建日志文件 </strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">touch ~/.msmtp.log</div></pre></td></tr></table></figure>
<h2 id="邮件发送测试"><a href="#邮件发送测试" class="headerlink" title="邮件发送测试"></a>邮件发送测试</h2><p>查看SMTP服务器是否支持认证的TLS加密</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># msmtp --host=smtp.126.com --serverinfo</div><div class="line">SMTP server at smtp.126.com (m15-111.126.com [220.181.15.111]), port 25:</div><div class="line">    126.com Anti-spam GT for Coremail System (126com[20140526])</div><div class="line">Capabilities:</div><div class="line">    PIPELINING:</div><div class="line">        Support for command grouping for faster transmission</div><div class="line">    STARTTLS:</div><div class="line">        Support for TLS encryption via the STARTTLS command</div><div class="line">    AUTH:</div><div class="line">        Supported authentication methods:</div><div class="line">        PLAIN LOGIN </div><div class="line">This server might advertise more or other capabilities when TLS is active.</div></pre></td></tr></table></figure>
<p>发送一个简单的邮件测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo &quot;hello world&quot; | mutt -s &quot;title&quot; 12456789@qq.com</div></pre></td></tr></table></figure>
<p>登录QQ邮箱，可以正常收到邮件，成功！</p>
<h2 id="四种发送方式"><a href="#四种发送方式" class="headerlink" title="四种发送方式"></a>四种发送方式</h2><p>1、<strong>带有主题，从文件中读取邮件的正文，并发送</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ mutt -s &quot;hello mutt&quot; user@example.com &lt; message.txt</div></pre></td></tr></table></figure>
<p>2、<strong>通过管道获取 <code>echo</code>命令输出作为邮件内容发送</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ echo &quot;hello mutt&quot; | mutt -s &quot;Testing mutt&quot; user@example.com</div></pre></td></tr></table></figure>
<p>3、<strong>发送带附件的邮件</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ echo &quot;hello mutt&quot; | mutt -s &quot;Testing mutt&quot; user@example.com -a test.tar.gz</div></pre></td></tr></table></figure>
<p>多个附件的话在每个附件前加上 <code>-a</code> 即可。</p>
<p>4、<strong>发送给多个收件人</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo &quot;hello mutt&quot; | mutt -s &quot;Testing muttt&quot; 111111@qq.com,222222@163.com -c 3333333@qq.com -b 44444444@126.com</div></pre></td></tr></table></figure>
<p>多个收件人之间用逗号隔开</p>
<p><strong>抄送</strong>，前面加参数 <code>-c</code> </p>
<p><strong>密送</strong>，前面加参数 <code>-b</code></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>1、出现如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">msmtp: authentication failed (method PLAIN)</div><div class="line">msmtp: server message: 530 Error: A secure connection is requiered(such as ssl). More information at http://service.mail.qq.com/cgi-bin/help?id=28</div><div class="line">msmtp: could not send mail (account default from /root/.msmtprc)</div><div class="line">Error sending message, child exited 77 (Insufficient permission.).</div><div class="line">Could not send the message.</div></pre></td></tr></table></figure>
<p>原因，没有权限进行操作</p>
<p>解决方法：</p>
<blockquote>
<p>登录网页邮箱，设置开启POP3/SMTP服务、IMAP/SMTP服务</p>
</blockquote>
<p><strong>参考资料</strong></p>
<p><a href="http://www.cnblogs.com/xiazh/archive/2011/04/15/2016966.html" target="_blank" rel="external">ubuntu下使用mutt和msmtp发送邮件的简单配置</a></p>
<p><a href="https://linux.cn/article-5502-1.html" target="_blank" rel="external">4个可以发送电子邮件的命令行工具</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[ubuntu下系统打包]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/11/ubuntu%E4%B8%8B%E7%B3%BB%E7%BB%9F%E6%89%93%E5%8C%85/</url>
      <content type="html"><![CDATA[<p>介绍在ubuntu下利用remastersys工具给系统打包成 iso 镜像的方法，在其他系统（centos、windows等）下方法类似，不在赘述。<br><a id="more"></a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>​    在初始环境下，经过多次反复安装和配置才组建完成的自己需要的环境，其中各种版本问题和依赖较多，安装过程繁杂，为了避免痛苦的安装过程轮回发生，需要给系统打个包，后续使用直接安装即可。</p>
<p>使用工具：Remastersys</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="1、依赖项的安装"><a href="#1、依赖项的安装" class="headerlink" title="1、依赖项的安装"></a>1、依赖项的安装</h3><p>依赖：<a href="ftp://ftp.gwdg.de/pub/linux/easyvdr/mirror/remastersys/ubuntu/remastersys/remastersys_3.0.4-2_all.deb" target="_blank" rel="external">remastersys_3.0.4-2_all.deb</a>，点击即可下载，也可以自己去去<a href="http://www.filewatcher.com/" target="_blank" rel="external">filewatcher</a>搜索下载。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install plymouth-x11 </div><div class="line">sudo dpkg -i remastersys_3.0.4-2_all.deb</div></pre></td></tr></table></figure>
<h3 id="2、Remastersys安装"><a href="#2、Remastersys安装" class="headerlink" title="2、Remastersys安装"></a>2、Remastersys安装</h3><p>2.1、添加安装源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo &quot;deb http://www.remastersys.com/ubuntu oneiric main&quot; &gt;&gt; /etc/apt/sources.list</div></pre></td></tr></table></figure>
<p>2.2、更新安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo install remastersys</div><div class="line">sudo remastersys</div></pre></td></tr></table></figure>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="语法格式"><a href="#语法格式" class="headerlink" title="语法格式"></a>语法格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo remastersys  backup|clean|dist  [cdfs|iso]  [filename.iso]</div></pre></td></tr></table></figure>
<p>两种打包方式：backup和dist</p>
<p>backup 是对整个系统完全打包，包含个人文件</p>
<p>dist 方式用做发行，不包含个人文件</p>
<blockquote>
<p>更多使用方法可以参考<a href="http://forum.ubuntu.org.cn/viewtopic.php?t=174719" target="_blank" rel="external">ubuntu论坛</a></p>
</blockquote>
<h3 id="系统打包"><a href="#系统打包" class="headerlink" title="系统打包"></a>系统打包</h3><p>3.1、产生一个只有档案系统的可发布的 livecd/dvd </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo remastersys dist cdfs</div></pre></td></tr></table></figure>
<p>3.2、产生一个可发布其名叫 filename.iso的ISO文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo remastersys dist iso filename.iso</div></pre></td></tr></table></figure>
<p>3.3、将产生的iso文件移动到安全的位置(如果不移动会被清除掉)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mv /home/remastersys/remastersys/filename.iso /root/</div></pre></td></tr></table></figure>
<p>3.4、清除由 remastersys产生的临时文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo remastersys clean</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux命令——history及其优化]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/11/Linux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94history%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<p>没啥好说的，就是查看历史输入的命令。这里引入两个优化点，一是历史记录带时间，二是可以统计汇总不同终端下的命令。<br><a id="more"></a></p>
<h2 id="命令格式"><a href="#命令格式" class="headerlink" title="命令格式"></a>命令格式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">history [n]</div><div class="line">history [-c]</div><div class="line">history [-raw] histfiles</div></pre></td></tr></table></figure>
<p>参数解释：</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>n</td>
<td>数字，列出最近的 n 条命令列表</td>
</tr>
<tr>
<td>-c</td>
<td>将目前的shell中的所有 history 内容全部清除</td>
</tr>
<tr>
<td>-a</td>
<td>将目前新增的history指令新增入 histfiles中，若没有➕histfiles，则预设写入 ~/.bash_history</td>
</tr>
<tr>
<td>-r</td>
<td>将histfiles的内容读到目前这个shell的history记忆中</td>
</tr>
<tr>
<td>-w</td>
<td>将目前的history记忆内容写入histfiles</td>
</tr>
</tbody>
</table>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>通常都是直接 <code>history</code> 然后通过管道配合 <code>more, less, tail, head</code> 来使用，其他复杂用法这里不作介绍。</p>
<p>如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">history | more</div><div class="line">history | tail -4</div></pre></td></tr></table></figure>
<h2 id="命令优化"><a href="#命令优化" class="headerlink" title="命令优化"></a>命令优化</h2><p><strong>出发点</strong>：在项目中，在不同终端下执行了一系列的命令后，翻不到历史记录，也没有汇总，也不知道每条命令的执行时间，所以需要对该命令进行优化。</p>
<p><strong>优化点</strong></p>
<ul>
<li>历史记录里面每条命令带有时间</li>
<li>所有终端命令都记录到history当中去</li>
</ul>
<blockquote>
<p>命令的汇总限同一个机器下的同一个用户</p>
</blockquote>
<p>注：以下设置方法适用ubuntu，其他系统类似。ubuntu上修改的文件是 <code>/etc/bash.bashrc</code>，mac上修改的文件是 <code>/etc/bashrc</code> 。</p>
<h3 id="添加日期"><a href="#添加日期" class="headerlink" title="添加日期"></a>添加日期</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim /etc/bash.bashrc</div></pre></td></tr></table></figure>
<p>在末尾添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">HISTTIMEFORMAT=&quot;%F %T &quot;</div><div class="line">export HISTTIMEFORMAT</div></pre></td></tr></table></figure>
<p>执行使生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source /etc/bash.bashrc</div></pre></td></tr></table></figure>
<p><strong>新开终端</strong>的时候，history命令的显示就带有日期了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">448  2017-01-11 11:42:01 vim /etc/bash.bashrc </div><div class="line">449  2017-01-11 11:42:45 source /etc/bash.bashrc </div><div class="line">450  2017-01-11 11:40:18 history</div><div class="line">451  2017-01-11 11:41:11 ls</div><div class="line">452  2017-01-11 11:41:12 pwd</div><div class="line">453  2017-01-11 11:41:35 history</div></pre></td></tr></table></figure>
<h3 id="不同终端命令汇总"><a href="#不同终端命令汇总" class="headerlink" title="不同终端命令汇总"></a>不同终端命令汇总</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim /etc/bash.bashrc</div></pre></td></tr></table></figure>
<p>在末尾添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># Avoid duplicates</div><div class="line">export HISTCONTROL=ignoredups:erasedups</div><div class="line"># When the shell exits, append to the history file instead of overwriting it</div><div class="line">shopt -s histappend</div><div class="line"></div><div class="line"># After each command, append to the history file and reread it</div><div class="line">export PROMPT_COMMAND=&quot;$&#123;PROMPT_COMMAND:+$PROMPT_COMMAND$&apos;\n&apos;&#125;history -a; history -c; history -r&quot;</div></pre></td></tr></table></figure>
<p>执行使生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source /etc/bash.bashrc</div></pre></td></tr></table></figure>
<p><strong>新开终端</strong>的时候就可以看到之前在该机器该用户下的所有命令操作。</p>
<h3 id="调整记录命令长度"><a href="#调整记录命令长度" class="headerlink" title="调整记录命令长度"></a>调整记录命令长度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim /etc/bash.bashrc</div></pre></td></tr></table></figure>
<p>在末尾添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export HISTSIZE=1000	# 该数字自己定</div></pre></td></tr></table></figure>
<p>执行使生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source /etc/bash.bashrc</div></pre></td></tr></table></figure>
<h3 id="剔除连续重复的条目"><a href="#剔除连续重复的条目" class="headerlink" title="剔除连续重复的条目"></a>剔除连续重复的条目</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim /etc/bash.bashrc</div></pre></td></tr></table></figure>
<p>将 HISTCONTROL 设置为 ignoredups</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Avoid duplicates</div><div class="line">export HISTCONTROL=ignoredups:erasedups</div></pre></td></tr></table></figure>
<p>执行使生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source /etc/bash.bashrc</div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux下利用screen进行屏幕协作]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/11/Linux%E4%B8%8B%E5%88%A9%E7%94%A8screen%E8%BF%9B%E8%A1%8C%E5%B1%8F%E5%B9%95%E5%8D%8F%E4%BD%9C/</url>
      <content type="html"><![CDATA[<p>Linux本身是支持多终端并行处理的，但是某些时候我们可能需要两个人同时处理同一个终端，比如远程协助定位问题等。screen正好能满足这个需求。<br><a id="more"></a></p>
<h2 id="screen的安装"><a href="#screen的安装" class="headerlink" title="screen的安装"></a>screen的安装</h2><p>debian和ubuntu下安装方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">apt-get install screen</div></pre></td></tr></table></figure>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>假设现在有a和b需要进行屏幕协作。</p>
<blockquote>
<p>他们需要登录同一台服务器/机器</p>
<p>必须使用相同的账户，不同账户之前不能屏幕协作</p>
</blockquote>
<p>第一步：a 在终端上运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">screen -S zp</div></pre></td></tr></table></figure>
<p><strong>注意</strong>：<code>S</code> 是大写</p>
<p>第二步：b 在终端上运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">screen -x zp</div></pre></td></tr></table></figure>
<p>这时两边的操作是实时而且同步的。</p>
<h2 id="退出协作"><a href="#退出协作" class="headerlink" title="退出协作"></a>退出协作</h2><p>在任意一个终端上输入命令：<code>exit</code> ，输出如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[screen is terminating]</div></pre></td></tr></table></figure>
<p>表示退出成功。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Linux命令——crontab]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/11/Linux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94crontab/</url>
      <content type="html"><![CDATA[<p>crond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程。通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。<br><a id="more"></a></p>
<h2 id="命令格式"><a href="#命令格式" class="headerlink" title="命令格式"></a>命令格式</h2><p>crontab使用格式说明</p>
<p><img src="http://ojet8bprn.bkt.clouddn.com/crontab%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E.png" alt="crontab格式说明"></p>
<p>如图所示：</p>
<ul>
<li>第1列分钟0～59</li>
<li>第2列小时0～23（0表示子夜）</li>
<li>第3列日1～31</li>
<li>第4列月1～12</li>
<li>第5列星期0～7（0和7表示星期天）</li>
<li>第6列要运行的命令</li>
</ul>
<p><strong>TIPS</strong></p>
<p>每一列之间用空格隔开。</p>
<p>破折号 <code>-</code> 表示一个整数范围。例如：“2-4”表示“2,3,4”。</p>
<p>逗号 <code>,</code> 表示枚举的意思，一个指定的集合。例如：“1,5,8”。</p>
<p>正向斜杠 <code>/</code> 表示间隔频率。例如：“8-20/2”表示从8点到20点每2小时执行一次。</p>
<p><code>/</code> 可以和 <code>*</code> 一起使用。例如<code>*/5</code>，如果用在minute字段，表示每五分钟执行一次。<code>* */1 * * *</code>  表示每一小时执行一次。</p>
<p><strong>命令格式</strong></p>
<p><code>crontab [-u user] file crontab [-u user][ -e | -l | -r ]</code></p>
<h2 id="选项详解"><a href="#选项详解" class="headerlink" title="选项详解"></a>选项详解</h2><ul>
<li>-u user：用来设定某个用户的crontab服务；</li>
<li>file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。</li>
<li>-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。</li>
<li>-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。</li>
<li>-r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。</li>
<li>-i：在删除用户的crontab文件时给确认提示。</li>
</ul>
<h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="一些命令"><a href="#一些命令" class="headerlink" title="一些命令"></a>一些命令</h3><p>1、创建crontab文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ crontab yourcron</div></pre></td></tr></table></figure>
<p>2、列出crontab文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ crontab -l</div><div class="line">0 */3 * * * command &gt;/dev/null 2&gt;&amp;1</div></pre></td></tr></table></figure>
<p>3、编辑crontab文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ crontab -e</div></pre></td></tr></table></figure>
<p>4、删除crontab文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$crontab -r</div></pre></td></tr></table></figure>
<p>5、ubuntu下启动、停止与重启cron</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$sudo /etc/init.d/cron start</div><div class="line">$sudo /etc/init.d/cron stop</div><div class="line">$sudo /etc/init.d/cron restart</div></pre></td></tr></table></figure>
<p>6、清理用户的邮件日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0 */3 * * * command &gt;/dev/null 2&gt;&amp;1</div></pre></td></tr></table></figure>
<p><code>&gt; /dev/null 2&gt;&amp;1</code> 表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。</p>
<blockquote>
<p>每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要</p>
</blockquote>
<h3 id="两种使用方式"><a href="#两种使用方式" class="headerlink" title="两种使用方式"></a>两种使用方式</h3><p><strong>一种是新建crontab任务，然后重启cron服务</strong></p>
<p>第一步、创建一个crontab文件</p>
<p>设置环境变量EDITOR。cron进程根据它来确定使用哪个编辑器编辑crontab文件。使用vi的话，在$HOME目录下的. profile文件中加入一行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">EDITOR=vi; export EDITOR</div></pre></td></tr></table></figure>
<p>创建一个比如名为 testcron 的文件，在其中加入自己想要执行的命令，如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0 */3 * * * command &gt;/dev/null 2&gt;&amp;1</div></pre></td></tr></table></figure>
<p>把这个新创建的文件作为cron命令的参数，用来创建crontab任务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ crontab testcron</div></pre></td></tr></table></figure>
<p>第二步、重启cron服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$sudo /etc/init.d/cron restart</div></pre></td></tr></table></figure>
<p><strong>失败！</strong></p>
<blockquote>
<p>试过多次，不知道为啥这种通过自己创建crontab文件的方式不行，查资料也未果。后续有机会再试试，就暂时先用后一种可用的方法吧。有知道的忘不吝指教。</p>
</blockquote>
<p><strong>另一种是把要执行的命令直接写入 <code>/etc/crontab</code> 中，然后重启cron服务</strong></p>
<p>第一步、直接编辑 /etc/crontab</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">vim /etc/crontab</div><div class="line">添加自己的任务</div><div class="line">* * * * * root sh myjob.sh &gt;/dev/null 2&gt;&amp;1</div></pre></td></tr></table></figure>
<p>第二步、重启cron服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$sudo /etc/init.d/cron restart</div></pre></td></tr></table></figure>
<p><strong>成功！</strong></p>
<p>如果需要停止该任务的话</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$sudo /etc/init.d/cron stop</div></pre></td></tr></table></figure>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>实例1：每1分钟执行一次myCommand</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">* * * * * myCommand</div></pre></td></tr></table></figure>
<p>实例2：每小时的第3和第15分钟执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">3,15 * * * * myCommand</div></pre></td></tr></table></figure>
<p>实例3：在上午8点到11点的第3和第15分钟执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">3,15 8-11 * * * myCommand</div></pre></td></tr></table></figure>
<p>实例4：每隔两天的上午8点到11点的第3和第15分钟执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">3,15 8-11 */2  *  * myCommand</div></pre></td></tr></table></figure>
<p>实例5：每周一上午8点到11点的第3和第15分钟执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">3,15 8-11 * * 1 myCommand</div></pre></td></tr></table></figure>
<p>实例6：每晚的21:30重启smb</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">30 21 * * * /etc/init.d/smb restart</div></pre></td></tr></table></figure>
<p>实例7：每月1、10、22日的4 : 45重启smb</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">45 4 1,10,22 * * /etc/init.d/smb restart</div></pre></td></tr></table></figure>
<p>实例8：每周六、周日的1 : 10重启smb</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">10 1 * * 6,0 /etc/init.d/smb restart</div></pre></td></tr></table></figure>
<p>实例9：每天18 : 00至23 : 00之间每隔30分钟重启smb</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0,30 18-23 * * * /etc/init.d/smb restart</div></pre></td></tr></table></figure>
<p>实例10：每星期六的晚上11 : 00 pm重启smb</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0 23 * * 6 /etc/init.d/smb restart</div></pre></td></tr></table></figure>
<p>实例11：每一小时重启smb</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">* */1 * * * /etc/init.d/smb restart</div></pre></td></tr></table></figure>
<p>实例12：晚上11点到早上7点之间，每隔一小时重启smb</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0 23-7 * * * /etc/init.d/smb restart</div></pre></td></tr></table></figure>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>1、如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件中。</p>
<p>2、新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。</p>
<p>3、<code>crontab -r</code> 将从Crontab目录（/var/spool/cron）中删除用户的Crontab文件，该用户的所有crontab都将被删除。</p>
<p>4、在crontab中 <code>%</code> 表示换行的意思。使用的时候必须进行转义%。比如经常使用的 <code>date ‘+%Y%m%d’</code> 在crontab里是不会执行的，应该换成 <code>date ‘+\%Y\%m\%d’’</code>。</p>
<p><strong>参考资料</strong>：<a href="http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/crontab.html" target="_blank" rel="external">crontab 定时任务</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Mac上hexo博客的搭建]]></title>
      <url>http://xiaqunfeng.github.io/2017/01/06/hexo%E7%9A%84%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<p>​    第一篇hexo博客，介绍了在mac上搭建hexo博客的全过程。包括：准备工作、本地站点的建立、启动和部署站点、创建新文章、发布到github、更换主题 和 绑定域名。<br><a id="more"></a></p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol>
<li><strong>github</strong></li>
<li><strong>homebrew</strong></li>
<li><strong>node.js 和 npm</strong></li>
<li><strong>hexo</strong></li>
</ol>
<h3 id="github"><a href="#github" class="headerlink" title="github"></a>github</h3><p>1、github账号</p>
<p>​    这部分略过，没有的话自己注册一个。然后将mac上的公钥添加到github上去。</p>
<p>2、建立仓库</p>
<p>​    严格按照 <code>git用户名</code>.<code>github</code>.<code>io</code> 来命名。我这里就叫：<code>xiaqunfeng.github.io</code> 。</p>
<h3 id="homebrew"><a href="#homebrew" class="headerlink" title="homebrew"></a>homebrew</h3><p>homebrew 是MAC OSX 上面用来安装 或者 卸载软件用的非常方面的一个软件。在终端上执行如下命令即可安装，参考<a href="http://brew.sh/index_zh-cn.html" target="_blank" rel="external">官网</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</div></pre></td></tr></table></figure>
<h3 id="node-js"><a href="#node-js" class="headerlink" title="node.js"></a>node.js</h3><p>不推荐使用 brew 安装node， 直接从<a href="https://nodejs.org/en/" target="_blank" rel="external">官网</a>下载，然后双击安装。我这里选择的版本是：v6.9.3 LTS。node.js 集成带有npm。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ node -v</div><div class="line">v6.9.3</div><div class="line"></div><div class="line">$ npm -v</div><div class="line">3.10.10</div></pre></td></tr></table></figure>
<h3 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo npm install hexo-cli -g</div></pre></td></tr></table></figure>
<p>安装完后</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ hexo -v</div><div class="line">hexo: 3.2.2</div><div class="line">hexo-cli: 1.0.2</div><div class="line">os: Darwin 16.1.0 darwin x64</div><div class="line">http_parser: 2.7.0</div><div class="line">node: 6.9.3</div><div class="line">v8: 5.1.281.89</div><div class="line">uv: 1.9.1</div><div class="line">zlib: 1.2.8</div><div class="line">ares: 1.10.1-DEV</div><div class="line">icu: 57.1</div><div class="line">modules: 48</div><div class="line">openssl: 1.0.2j</div></pre></td></tr></table></figure>
<h2 id="hexo命令行使用"><a href="#hexo命令行使用" class="headerlink" title="hexo命令行使用"></a>hexo命令行使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">常用命令：</div><div class="line">hexo help			#查看帮助</div><div class="line">hexo init 			#初始化一个目录</div><div class="line">hexo new &quot;postName&quot; 		#新建文章</div><div class="line">hexo new page &quot;pageName&quot; 	#新建页面</div><div class="line">hexo generate 			#生成网页，可以在 public 目录查看整个网站的文件</div><div class="line">hexo server 			#本地预览，&apos;Ctrl+C&apos;关闭</div><div class="line">hexo deploy 			#部署.deploy目录</div><div class="line">hexo clean 			#清除缓存，强烈建议每次执行命令前先清理缓存，每次部署前先删除 .deploy 文件夹</div><div class="line"></div><div class="line">简写：</div><div class="line">hexo n == hexo new</div><div class="line">hexo g == hexo generate</div><div class="line">hexo s == hexo server</div><div class="line">hexo d == hexo deploy</div></pre></td></tr></table></figure>
<h2 id="建立本地站点"><a href="#建立本地站点" class="headerlink" title="建立本地站点"></a>建立本地站点</h2><p>执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。（文件夹不需要提前建好，会自行创建）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo init &lt;folder&gt;</div><div class="line">$ cd &lt;folder&gt;</div><div class="line">$ npm install</div></pre></td></tr></table></figure>
<p>新建文件夹目录如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── .deploy 		#需要部署的文件</div><div class="line">├── node_modules 	#Hexo插件</div><div class="line">├── public 		#生成的静态网页文件</div><div class="line">├── scaffolds 		#模板</div><div class="line">├── source 		#博客正文和其他源文件</div><div class="line">| ├── _drafts 		#草稿(刚开始没有)</div><div class="line">| └── _posts 		#文章</div><div class="line">├── themes 		#主题</div><div class="line">├── _config.yml 	#全局配置文件</div><div class="line">└── package.json</div></pre></td></tr></table></figure>
<h2 id="启动站点"><a href="#启动站点" class="headerlink" title="启动站点"></a>启动站点</h2><p>执行 hexo server 启动站点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo s</div><div class="line">INFO  Start processing</div><div class="line">INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</div></pre></td></tr></table></figure>
<p>在浏览器中输入：<a href="http://localhost:4000" target="_blank" rel="external">http://localhost:4000</a> 就可以看到站点了。</p>
<h2 id="部署站点"><a href="#部署站点" class="headerlink" title="部署站点"></a>部署站点</h2><p>编辑文件 <code>_config.yml</code> ，修改如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">   type: git</div><div class="line">   repository: https://github.com/xiaqunfeng/xiaqunfeng.github.io.git</div><div class="line">   branch: master</div></pre></td></tr></table></figure>
<p><strong>注意</strong>：冒号后面要留 <strong>空格</strong>！！</p>
<h2 id="创建新文章"><a href="#创建新文章" class="headerlink" title="创建新文章"></a>创建新文章</h2><p><strong>方法1</strong>、<code>hexo n</code>创建新文章</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hexo n &quot;我的第一篇hexo博客&quot;</div><div class="line">INFO  Created: ~/hexo-blog/source/_posts/我的第一篇hexo博客.md</div></pre></td></tr></table></figure>
<p>然后用编辑器打开编辑即可。</p>
<p><strong>方法2</strong>、直接在 <code>source/_posts</code> 中新建一个md文件，进行编辑</p>
<p>在hexo-blog文件夹下执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hexo g</div><div class="line">hexo s</div></pre></td></tr></table></figure>
<p>然后在浏览器中输入<code>http://localhost:4000</code>就可以看到新加的文章了。</p>
<h2 id="发布"><a href="#发布" class="headerlink" title="发布"></a>发布</h2><p>执行 <code>hexo d</code> ，发现如下问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hexo d</div><div class="line">ERROR Deployer not found: git</div></pre></td></tr></table></figure>
<p>解决方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-deployer-git --save</div><div class="line">hexo-site@0.0.0 /Users/xiaqunfeng/hexo-blog</div><div class="line">└── hexo-deployer-git@0.2.0</div></pre></td></tr></table></figure>
<p>再执行 <code>hexo d</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ hexo d</div><div class="line">INFO  Deploying: git</div><div class="line">INFO  Clearing .deploy_git folder...</div><div class="line">INFO  Copying files from public folder...</div><div class="line">On branch master</div><div class="line">nothing to commit, working tree clean</div><div class="line">Username for &apos;https://github.com&apos;: xiaqunfeng</div><div class="line">Password for &apos;https://xiaqunfeng@github.com&apos;: </div><div class="line">To https://github.com/xiaqunfeng/xiaqunfeng.github.io.git</div><div class="line"> * [new branch]      HEAD -&gt; master</div><div class="line">Branch master set up to track remote branch master from https://github.com/xiaqunfeng/xiaqunfeng.github.io.git.</div><div class="line">INFO  Deploy done: git</div></pre></td></tr></table></figure>
<p>输入github的用户名和密码即可。此时，博客已经完全搭建起来了。</p>
<p>在浏览器中输入：<a href="https://xiaqunfeng.github.io/">https://xiaqunfeng.github.io/</a> 即可访问。</p>
<p>预览如下： <img src="http://ojet8bprn.bkt.clouddn.com/hexo-new.png" alt=""></p>
<h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>自己google一下hexo主题，选一个自己喜欢的，然后git clone下来。比如 Next 主题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git clone https://github.com/iissnan/hexo-theme-next.git</div></pre></td></tr></table></figure>
<p>然后打开_config.yml文件，替换其中的 <code>theme</code> 属性，默认为 <code>landscape</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">theme: next</div></pre></td></tr></table></figure>
<h3 id="本地调试"><a href="#本地调试" class="headerlink" title="本地调试"></a>本地调试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hexo g #生成静态页面，生成的内容在public文件夹下</div><div class="line">$ hexo s #启动本地服务，进行文章预览调试。hexo s --debug 命令可以用来调试</div></pre></td></tr></table></figure>
<h3 id="发布到github"><a href="#发布到github" class="headerlink" title="发布到github"></a>发布到github</h3><p>1、清理之前生成的内容，即public文件。</p>
<p><strong>注意</strong>：这一步必须要，不然有时因为缓存问题，服务器更新不了主题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo clean</div></pre></td></tr></table></figure>
<p>2、生成静态文件并部署到github</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hexo g #生成静态文件</div><div class="line">$ hexo d #部署到github</div></pre></td></tr></table></figure>
<p>上面两个命令可以合并为一个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo d -g #在部署前先生成</div></pre></td></tr></table></figure>
<h2 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h2><blockquote>
<p>我暂时还没购买和绑定域名，就暂时先简单介绍一下，后续再来补充完善。</p>
</blockquote>
<p>1、购买域名</p>
<p>某乎<a href="https://www.zhihu.com/question/19551906" target="_blank" rel="external">这里</a>有各种推荐，自己可以参考的看看。</p>
<p>godaddy地址: <a href="https://www.godaddy.com/" target="_blank" rel="external">https://www.godaddy.com</a></p>
<p>阿里云域名地址: <a href="http://wanwang.aliyun.com/" target="_blank" rel="external">http://wanwang.aliyun.com</a></p>
<p>2、到自己的gitHubPages的ip地址</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ping http://xiaqunfeng.github.io</div><div class="line">PING github.map.fastly.net (151.101.192.133): 56 data bytes</div><div class="line">64 bytes from 151.101.192.133: icmp_seq=0 ttl=49 time=62.670 ms</div><div class="line">64 bytes from 151.101.192.133: icmp_seq=1 ttl=49 time=65.062 ms</div><div class="line">...</div></pre></td></tr></table></figure>
<p>这里IP地址为：<code>151.101.192.133</code></p>
<p>3、域名绑定IP地址</p>
<p>登录自己购买域名的地方，找到域名相关选项</p>
<p>添加域名</p>
<p>添加记录：一个主机记录 <code>@</code>， 一个为<code>www</code>，记录值都是博客主页对应的ip。 </p>
<p>等待生效，最迟72小时生效，然后就可通过域名浏览你的博客主页了。</p>
]]></content>
    </entry>
    
  
  
</search>
